{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51114977",
   "metadata": {},
   "source": [
    "# Testing Retrieval QA Chains\n",
    "In this notebooks we'll evaluate a retrieval QA chain on a set of edge case scenarios. We'll compare different chains to see which parameters and architectures are the most performant on these edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "25ad9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904bd0da",
   "metadata": {},
   "source": [
    "# Retrieval test cases\n",
    "Each test case should contain a list of corpus texts to be stored, optional metadata for the texts, the query used for testing, and the expected chain output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "fa761564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Need to retrieve many documents\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_retrieve_many_docs_test_case(num_samples=5, num_total=100):\n",
    "    \"\"\"Test if retriever can return as many documents as query requires.\"\"\"\n",
    "    text_template = \"On {date} the peak temperature was {temp} degrees\"\n",
    "    dates = pd.date_range(start=\"01-01-2023\", freq='D', periods=num_total).astype(str)\n",
    "    temps = [str(random.randint(50, 80)) for _ in range(len(dates))]\n",
    "    sample_idxs = random.sample(range(len(dates)), k=num_samples)\n",
    "\n",
    "    texts = [text_template.format(date=d, temp=t) for d, t in zip(dates, temps)]\n",
    "    query = f\"What were the peak temperatures on {', '.join([dates[i] for i in sample_idxs])}?\"\n",
    "    expected = f\"The peak temperatures were {', '.join(temps[i] for i in sample_idxs)} degrees\"\n",
    "    def retriever_check(retrieved_docs) -> bool:\n",
    "        retrieved_texts = set(d.page_content for d in retrieved_docs)\n",
    "        return all(texts[i] in retrieved_texts for i in sample_idxs)\n",
    "    return {\n",
    "        \"name\": f\"Retrieve {num_samples} documents\",\n",
    "        \"texts\": texts,\n",
    "        \"query\": query,\n",
    "        \"expected\": expected,\n",
    "        \"retriever_check\": retriever_check\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "9b9c6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Redundant docs\n",
    "def get_redundant_docs_test_case():\n",
    "    \"\"\"Test if retriever can handle many redundant documents.\"\"\"\n",
    "    texts = [\n",
    "        \"OpenAI announces the release of GPT-5\",\n",
    "        \"GPT-5 released by OpenAI\",\n",
    "        \"The next-generation OpenAI GPT model is here\",\n",
    "        \"GPT-5: OpenAI's next model is the biggest yet\",\n",
    "        \"Sam Altman's OpenAI comes out with new GPT-5 model\",\n",
    "        \"GPT-5 is here. What you need to know about the OpenAI model\",\n",
    "        \"OpenAI announces ChatGPT successor GPT-5\",\n",
    "        \"5 jaw-dropping things OpenAI's GPT-5 can do that ChatGPT couldn't\",\n",
    "        \"OpenAI's GPT-5 Is Exciting and Scary\",\n",
    "        \"OpenAI announces GPT-5, the new generation of AI\",\n",
    "        \"OpenAI says new model GPT-5 is more creative and less\",\n",
    "        \"Meta open sources new AI model, largest yet\",\n",
    "    ]\n",
    "    query = \"What companies have recently released new models?\"\n",
    "    expected = \"OpenAI and Meta have recently released new models\"\n",
    "    def retriever_check(retrieved_docs) -> bool:\n",
    "        return any(\"Meta\" in d.page_content for d in retrieved_docs)\n",
    "    return {\n",
    "        \"name\": \"Redundant documents\",\n",
    "        \"texts\": texts,\n",
    "        \"query\": query,\n",
    "        \"expected\": expected,\n",
    "        \"retriever_check\": retriever_check\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "828cb2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Information about entity split across documents using different entity names\n",
    "def get_disambiguate_and_combine_test_case():\n",
    "    texts = [\n",
    "        \"The founder of ReallyCoolAICompany LLC is from Louisville, Kentucky.\",\n",
    "        \"Melissa Harkins, founder of ReallyCoolAICompany LLC, said in a recent interview that she will be stepping down as CEO.\",\n",
    "        state_of_union,\n",
    "    ]\n",
    "    query = \"Where is Melissa Harkins from?\"\n",
    "    expected = \"Melissa Harkins is from Louisville, Kentucky\"\n",
    "    def retriever_check(retrieved_docs) -> bool:\n",
    "        for keyword in (\"Melissa Harkins\", \"Louisville, Kentucky\"):\n",
    "            found = False\n",
    "            for doc in retrieved_docs:\n",
    "                if keyword in doc.page_content:\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                return False\n",
    "        return True\n",
    "            \n",
    "    return {\n",
    "        \"name\": \"Entity disambiguation\",\n",
    "        \"texts\": texts,\n",
    "        \"query\": query,\n",
    "        \"expected\": expected,\n",
    "        \"retriever_check\": retriever_check\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "8f68de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Metadata question (temporal)\n",
    "def get_temporal_test_case(num_total=100):\n",
    "    scores = [random.randint(1, 4) for _ in range(num_total - 1)] + [4]\n",
    "    dates = pd.date_range(start=\"01-01-2023\", freq='D', periods=num_total).astype(str)\n",
    "    texts = [\n",
    "        f\"Daily log: My energy levels were a {score} out of 4 today\" for score in scores\n",
    "    ]\n",
    "    metadatas = [{\"date\": d} for d in dates]\n",
    "    query = \"When was the first time I reported an energy level of 4?\"\n",
    "    expected = f\"The first time was {dates[scores.index(4)]}\"\n",
    "    def retriever_check(retrieved_docs) -> bool:\n",
    "        retrieved_dates = set(d.metadata.get('date') for d in retrieved_docs)\n",
    "        return dates[scores.index(4)] in retrieved_dates\n",
    "    return {\n",
    "        \"name\": \"Metadata question (temporal information)\",\n",
    "        \"texts\": texts,\n",
    "        \"query\": query,\n",
    "        \"expected\": expected,\n",
    "        \"metadatas\": metadatas,\n",
    "        \"retriever_check\": retriever_check\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "251fba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Store a single text which updates a fact multiple times.\n",
    "def get_revised_fact_test_case():\n",
    "    split_sou = state_of_union.split(\". \")\n",
    "    len_split_sou = len(split_sou)\n",
    "    updates = [\n",
    "        \"We are receiving reports of a magnitude 10 earthquake in Japan\",\n",
    "        \"The latest reports are that the earthquake that has hit Japan is actually of magnitude 8.5\",\n",
    "        \"Now the earthquake in Japan has been downgraded to magnitude 7\",\n",
    "        \"Looks like the earthquake is back up to an 8\",\n",
    "        \"The latest news is that the earthquake was of magnitude 3\",\n",
    "        \"The Japanese earthquake is now being recorded as magnitude 5\"\n",
    "    ]\n",
    "    chunk_size = len_split_sou // len(updates)\n",
    "    sou_chunks = [split_sou[idx: idx + chunk_size] for idx in range(0, len_split_sou, chunk_size)]\n",
    "    for update, chunk in zip(updates, sou_chunks):\n",
    "        chunk.append(update)\n",
    "    texts = [\". \".join([s for chunk in sou_chunks for s in chunk])]\n",
    "    query = \"What is the latest reported magnitude of the earthquake in Japan?\"\n",
    "    expected = \"The latest reported magnitude of the earthquake is 5\"\n",
    "    def retriever_check(retrieved_docs) -> bool:\n",
    "        retrieved_texts = [d.page_content for d in retrieved_docs]\n",
    "        return any(\"magnitude 5\" in t for t in retrieved_texts)\n",
    "    return {\n",
    "        \"name\": \"Revised statements\",\n",
    "        \"texts\": texts,\n",
    "        \"query\": query,\n",
    "        \"expected\": expected,\n",
    "        \"retriever_check\": retriever_check,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "a48ce619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. A text with many facts and ask about only one of them\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "def get_single_fact_test_case():\n",
    "    loader = TextLoader('../docs/modules/state_of_the_union.txt')\n",
    "    text = loader.load()[0].page_content\n",
    "    text += \" The color of the cat is purple.\"\n",
    "    query = \"What color is the cat?\"\n",
    "    expected = \"The cat is purple\"\n",
    "    def retriever_check(retrieved_docs) -> bool:\n",
    "        retrieved_texts = [d.page_content for d in retrieved_docs]\n",
    "        return any(\"purple\" in t for t in retrieved_texts)\n",
    "    return {\n",
    "        \"name\": \"One fact in long text\",\n",
    "        \"texts\": [text],\n",
    "        \"query\": query,\n",
    "        \"expected\": expected,\n",
    "        \"retriever_check\": retriever_check\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2fb192",
   "metadata": {},
   "source": [
    "# Candidate retrieval systems\n",
    "Now that we've defined some test cases, let's create a couple candidate retrieval systems to evaluate and compare. For this we'll need to define a function that returns a Retriever given a set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "d6d7c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "02011a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"get-ur-own-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "0c762c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_candidates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "51b071ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla vectorestore retriever\n",
    "def get_retriever(documents):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    documents = text_splitter.split_documents(documents)\n",
    "    docsearch = Chroma.from_documents(documents, OpenAIEmbeddings())\n",
    "    return docsearch.as_retriever(search_kwargs={\"k\": min(4, len(texts))})\n",
    "candidate = {\n",
    "    \"params\": {\"search\": \"similarity\", \"k\": 4, \"chunk_size\": 1000},\n",
    "    \"get_retriever\": get_retriever,\n",
    "}\n",
    "retrieval_candidates.append(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "fe611bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever with larger context, uses max marginal relevance, tuned prompt, more granular chunking\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "retrieval_prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know. Don't try to make up an answer. If the context contains conflicting pieces of information, say that the answer cannot be determined from the context.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "improved_retrieval_prompt = PromptTemplate(\n",
    "    template=retrieval_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "def get_retriever(documents):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n",
    "    documents = text_splitter.split_documents(documents)\n",
    "    for doc in documents:\n",
    "        doc.page_content = f\"Document metadata: {doc.metadata}\\n\\n\" + doc.page_content\n",
    "    docsearch = Chroma.from_documents(documents, OpenAIEmbeddings())\n",
    "    return docsearch.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": min(6, len(texts)), \"fetch_k\": min(20, len(texts))})\n",
    "\n",
    "candidate = {\n",
    "    \"params\": {\"search\": \"mmr\", \"k\": 6, \"chunk_size\": 200, \"metadata\": \"prepend to content\", \"prompt\": \"improved\"},\n",
    "    \"get_retriever\": get_retriever,\n",
    "    \"qa_kwargs\": {\"prompt\": improved_retrieval_prompt}\n",
    "}\n",
    "retrieval_candidates.append(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "7b2c64b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhoAmIResponse(username='805b516', user_label='default', projectname='6ddd519')"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import pinecone\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "from langchain.retrievers import PineconeHybridSearchRetriever\n",
    "\n",
    "# os.environ[\"PINECONE_API_KEY\"] = \"get-ur-own-api-key\"\n",
    "# os.environ[\"PINECONE_ENVIRONMENT\"] = \"the-moon\"\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "env = os.getenv(\"PINECONE_ENVIRONMENT\")\n",
    "\n",
    "pinecone.init(api_key=api_key, enviroment=env)\n",
    "\n",
    "def get_retriever(documents):\n",
    "    timestamp = int(datetime.datetime.now().timestamp())\n",
    "    index_name = f\"hybrid-search-{timestamp}\"\n",
    "\n",
    "    # create the index\n",
    "    pinecone.create_index(\n",
    "       name = index_name,\n",
    "       dimension = 1536,  # dimensionality of dense model\n",
    "       metric = \"dotproduct\",  # sparse values supported only for dotproduct\n",
    "       pod_type = \"s1\",\n",
    "       metadata_config={\"indexed\": []}  # see explaination above\n",
    "    )\n",
    "    index = pinecone.Index(index_name)\n",
    "    # or from pinecone_text.sparse import SpladeEncoder if you wish to work with SPLADE\n",
    "\n",
    "    # use default tf-idf values\n",
    "    bm25_encoder = BM25Encoder().default()\n",
    "    retriever = PineconeHybridSearchRetriever(embeddings=embeddings, sparse_encoder=bm25_encoder, index=index)\n",
    "    \n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    documents = text_splitter.split_documents(documents)\n",
    "    for doc in documents:\n",
    "        doc.page_content = f\"Document metadata: {doc.metadata}\\n\\n\" + doc.page_content\n",
    "    retriever.add_texts([doc.page_content for doc in documents])\n",
    "    return retriever\n",
    "\n",
    "def cleanup_retriever(retriever):\n",
    "    index_name = retriever.index.configuration.server_variables['index_name']\n",
    "    pinecone.delete_index(index_name)\n",
    "\n",
    "candidate = {\n",
    "    \"params\": {\"search\": \"pinecone hybrid\", \"k\": 4, \"chunk_size\": 100},\n",
    "    \"get_retriever\": get_retriever,\n",
    "    \"cleanup_retriever\": cleanup_retriever\n",
    "}\n",
    "retrieval_candidates.append(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "037ae4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "eval_template = \"\"\"You are a teacher grading a quiz.\n",
    "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "STUDENT ANSWER: student's answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: CORRECT or INCORRECT here\n",
    "\n",
    "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. The student answer is correct if and only if it contains all of the infrormation in the true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
    "\n",
    "QUESTION: {query}\n",
    "STUDENT ANSWER: {result}\n",
    "TRUE ANSWER: {answer}\n",
    "GRADE:\"\"\"\n",
    "eval_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"result\", \"answer\"], template=eval_template\n",
    ")\n",
    "def default_evaluate(example, prediction) -> bool:\n",
    "    \"\"\"Return True if the prediction is correct.\"\"\"\n",
    "    eval_chain = QAEvalChain.from_llm(OpenAI(temperature=0), prompt=eval_prompt)\n",
    "    grades = eval_chain.evaluate([example], [prediction])\n",
    "    return grades[0]['text'].strip().upper() == \"CORRECT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a690dd",
   "metadata": {},
   "source": [
    "# Run test cases against each retrieval system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "c71e4897",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    get_retrieve_many_docs_test_case(),\n",
    "    get_revised_fact_test_case(),\n",
    "    get_single_fact_test_case(),\n",
    "    get_redundant_docs_test_case(),\n",
    "    get_disambiguate_and_combine_test_case(),\n",
    "    get_temporal_test_case(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "53369aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "# Suppress chroma warnings about transient DB.\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "test_results = []\n",
    "for retrieval_candidate in retrieval_candidates:\n",
    "    res = {\"params\": retrieval_candidate[\"params\"], \"test_cases\": {}}\n",
    "    for tc in test_cases:\n",
    "        try:\n",
    "            texts = tc[\"texts\"]\n",
    "            metadatas = tc.get(\"metadatas\", [{}] * len(texts))\n",
    "            docs = [Document(page_content=text, metadata=metadata) for text, metadata in zip(texts, metadatas)]\n",
    "            retriever = retrieval_candidate[\"get_retriever\"](docs)\n",
    "            qa = RetrievalQA.from_llm(llm=OpenAI(), retriever=retriever, **retrieval_candidate.get('qa_kwargs', {}))\n",
    "            example = {\"query\": tc[\"query\"], \"answer\": tc[\"expected\"]}\n",
    "            prediction = qa.apply([example])[0]\n",
    "            retrieved_docs = retriever.get_relevant_documents(tc[\"query\"])\n",
    "\n",
    "            res[\"test_cases\"][tc[\"name\"]] = {\n",
    "                \"pass\": tc.get(\"evaluate\", default_evaluate)(example, prediction),\n",
    "                \"prediction\": prediction,\n",
    "                \"retriever_pass\": tc['retriever_check'](retrieved_docs),\n",
    "                \"retrieved_docs\": retrieved_docs\n",
    "            }\n",
    "            if \"cleanup_retriever\" in retrieval_candidate:\n",
    "                retrieval_candidate[\"cleanup_retriever\"](retriever)\n",
    "        except:\n",
    "            continue\n",
    "    test_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "8a91ecf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3341f_row0_col1, #T_3341f_row0_col3, #T_3341f_row0_col6, #T_3341f_row1_col1, #T_3341f_row1_col2, #T_3341f_row1_col3, #T_3341f_row1_col5, #T_3341f_row1_col6, #T_3341f_row2_col1 {\n",
       "  background-color: mistyrose;\n",
       "}\n",
       "#T_3341f_row0_col2, #T_3341f_row0_col4, #T_3341f_row0_col5, #T_3341f_row1_col4, #T_3341f_row2_col2, #T_3341f_row2_col4, #T_3341f_row2_col5, #T_3341f_row2_col6 {\n",
       "  background-color: honeydew;\n",
       "}\n",
       "#T_3341f_row2_col3 {\n",
       "  background-color: whitesmoke;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3341f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3341f_level0_col0\" class=\"col_heading level0 col0\" >System</th>\n",
       "      <th id=\"T_3341f_level0_col1\" class=\"col_heading level0 col1\" >Retrieve 5 documents</th>\n",
       "      <th id=\"T_3341f_level0_col2\" class=\"col_heading level0 col2\" >Revised statements</th>\n",
       "      <th id=\"T_3341f_level0_col3\" class=\"col_heading level0 col3\" >One fact in long text</th>\n",
       "      <th id=\"T_3341f_level0_col4\" class=\"col_heading level0 col4\" >Redundant documents</th>\n",
       "      <th id=\"T_3341f_level0_col5\" class=\"col_heading level0 col5\" >Entity disambiguation</th>\n",
       "      <th id=\"T_3341f_level0_col6\" class=\"col_heading level0 col6\" >Metadata question (temporal information)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3341f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_3341f_row0_col0\" class=\"data row0 col0\" >{'search': 'similarity', 'k': 4, 'chunk_size': 1000}</td>\n",
       "      <td id=\"T_3341f_row0_col1\" class=\"data row0 col1\" >False</td>\n",
       "      <td id=\"T_3341f_row0_col2\" class=\"data row0 col2\" >True</td>\n",
       "      <td id=\"T_3341f_row0_col3\" class=\"data row0 col3\" >False</td>\n",
       "      <td id=\"T_3341f_row0_col4\" class=\"data row0 col4\" >True</td>\n",
       "      <td id=\"T_3341f_row0_col5\" class=\"data row0 col5\" >True</td>\n",
       "      <td id=\"T_3341f_row0_col6\" class=\"data row0 col6\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3341f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_3341f_row1_col0\" class=\"data row1 col0\" >{'search': 'mmr', 'k': 6, 'chunk_size': 200, 'metadata': 'prepend to content', 'prompt': 'improved'}</td>\n",
       "      <td id=\"T_3341f_row1_col1\" class=\"data row1 col1\" >False</td>\n",
       "      <td id=\"T_3341f_row1_col2\" class=\"data row1 col2\" >False</td>\n",
       "      <td id=\"T_3341f_row1_col3\" class=\"data row1 col3\" >False</td>\n",
       "      <td id=\"T_3341f_row1_col4\" class=\"data row1 col4\" >True</td>\n",
       "      <td id=\"T_3341f_row1_col5\" class=\"data row1 col5\" >False</td>\n",
       "      <td id=\"T_3341f_row1_col6\" class=\"data row1 col6\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3341f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_3341f_row2_col0\" class=\"data row2 col0\" >{'search': 'pinecone hybrid', 'k': 4, 'chunk_size': 100}</td>\n",
       "      <td id=\"T_3341f_row2_col1\" class=\"data row2 col1\" >False</td>\n",
       "      <td id=\"T_3341f_row2_col2\" class=\"data row2 col2\" >True</td>\n",
       "      <td id=\"T_3341f_row2_col3\" class=\"data row2 col3\" >nan</td>\n",
       "      <td id=\"T_3341f_row2_col4\" class=\"data row2 col4\" >True</td>\n",
       "      <td id=\"T_3341f_row2_col5\" class=\"data row2 col5\" >True</td>\n",
       "      <td id=\"T_3341f_row2_col6\" class=\"data row2 col6\" >True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x3cb411e80>"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize results\n",
    "import pandas as pd\n",
    "\n",
    "test_result_df = pd.DataFrame([{\"System\": f\"{res['params']}\", **{k: v['pass'] for k, v in res[\"test_cases\"].items()}} for res in test_results])\n",
    "\n",
    "def highlight(s):\n",
    "    res = []\n",
    "    for s_ in s:\n",
    "        if pd.isnull(s_):\n",
    "            res.append('background-color: whitesmoke')\n",
    "        elif s_:\n",
    "            res.append('background-color: honeydew')\n",
    "        else:\n",
    "            res.append('background-color: mistyrose')\n",
    "    return res\n",
    "\n",
    "test_result_df.style.apply(highlight, subset=test_result_df.columns.drop(\"System\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "4863ca2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_40db4_row0_col1, #T_40db4_row0_col5, #T_40db4_row0_col6, #T_40db4_row1_col1, #T_40db4_row1_col2, #T_40db4_row1_col5, #T_40db4_row1_col6, #T_40db4_row2_col1, #T_40db4_row2_col5, #T_40db4_row2_col6 {\n",
       "  background-color: mistyrose;\n",
       "}\n",
       "#T_40db4_row0_col2, #T_40db4_row0_col3, #T_40db4_row0_col4, #T_40db4_row1_col3, #T_40db4_row1_col4, #T_40db4_row2_col2, #T_40db4_row2_col4 {\n",
       "  background-color: honeydew;\n",
       "}\n",
       "#T_40db4_row2_col3 {\n",
       "  background-color: whitesmoke;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_40db4\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_40db4_level0_col0\" class=\"col_heading level0 col0\" >System</th>\n",
       "      <th id=\"T_40db4_level0_col1\" class=\"col_heading level0 col1\" >Retrieve 5 documents</th>\n",
       "      <th id=\"T_40db4_level0_col2\" class=\"col_heading level0 col2\" >Revised statements</th>\n",
       "      <th id=\"T_40db4_level0_col3\" class=\"col_heading level0 col3\" >One fact in long text</th>\n",
       "      <th id=\"T_40db4_level0_col4\" class=\"col_heading level0 col4\" >Redundant documents</th>\n",
       "      <th id=\"T_40db4_level0_col5\" class=\"col_heading level0 col5\" >Entity disambiguation</th>\n",
       "      <th id=\"T_40db4_level0_col6\" class=\"col_heading level0 col6\" >Metadata question (temporal information)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_40db4_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_40db4_row0_col0\" class=\"data row0 col0\" >{'search': 'similarity', 'k': 4, 'chunk_size': 1000}</td>\n",
       "      <td id=\"T_40db4_row0_col1\" class=\"data row0 col1\" >False</td>\n",
       "      <td id=\"T_40db4_row0_col2\" class=\"data row0 col2\" >True</td>\n",
       "      <td id=\"T_40db4_row0_col3\" class=\"data row0 col3\" >True</td>\n",
       "      <td id=\"T_40db4_row0_col4\" class=\"data row0 col4\" >True</td>\n",
       "      <td id=\"T_40db4_row0_col5\" class=\"data row0 col5\" >False</td>\n",
       "      <td id=\"T_40db4_row0_col6\" class=\"data row0 col6\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_40db4_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_40db4_row1_col0\" class=\"data row1 col0\" >{'search': 'mmr', 'k': 6, 'chunk_size': 200, 'metadata': 'prepend to content', 'prompt': 'improved'}</td>\n",
       "      <td id=\"T_40db4_row1_col1\" class=\"data row1 col1\" >False</td>\n",
       "      <td id=\"T_40db4_row1_col2\" class=\"data row1 col2\" >False</td>\n",
       "      <td id=\"T_40db4_row1_col3\" class=\"data row1 col3\" >True</td>\n",
       "      <td id=\"T_40db4_row1_col4\" class=\"data row1 col4\" >True</td>\n",
       "      <td id=\"T_40db4_row1_col5\" class=\"data row1 col5\" >False</td>\n",
       "      <td id=\"T_40db4_row1_col6\" class=\"data row1 col6\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_40db4_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_40db4_row2_col0\" class=\"data row2 col0\" >{'search': 'pinecone hybrid', 'k': 4, 'chunk_size': 100}</td>\n",
       "      <td id=\"T_40db4_row2_col1\" class=\"data row2 col1\" >False</td>\n",
       "      <td id=\"T_40db4_row2_col2\" class=\"data row2 col2\" >True</td>\n",
       "      <td id=\"T_40db4_row2_col3\" class=\"data row2 col3\" >nan</td>\n",
       "      <td id=\"T_40db4_row2_col4\" class=\"data row2 col4\" >True</td>\n",
       "      <td id=\"T_40db4_row2_col5\" class=\"data row2 col5\" >False</td>\n",
       "      <td id=\"T_40db4_row2_col6\" class=\"data row2 col6\" >False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x42acd2910>"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_result_df = pd.DataFrame([{\"System\": f\"{res['params']}\", **{k: v['retriever_pass'] for k, v in res[\"test_cases\"].items()}} for res in test_results])\n",
    "retriever_result_df.style.apply(highlight, subset=test_result_df.columns.drop(\"System\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "51b22a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'search': 'pinecone hybrid', 'k': 4, 'chunk_size': 100},\n",
       " 'test_cases': {'Retrieve 5 documents': {'pass': False,\n",
       "   'prediction': {'query': 'What were the peak temperatures on 2023-02-12, 2023-02-28, 2023-03-31, 2023-04-07, 2023-01-16?',\n",
       "    'answer': 'The peak temperatures were 55, 79, 52, 79, 54 degrees',\n",
       "    'result': \" The peak temperatures on 2023-02-12, 2023-02-28, 2023-03-31, 2023-04-07, and 2023-01-16 were 55 degrees, 79 degrees, 52 degrees, 79 degrees, and I don't know, respectively.\"},\n",
       "   'retriever_pass': False,\n",
       "   'retrieved_docs': [Document(page_content='Document metadata: {}\\n\\nOn 2023-03-31 the peak temperature was 52 degrees', metadata={}),\n",
       "    Document(page_content='Document metadata: {}\\n\\nOn 2023-02-12 the peak temperature was 55 degrees', metadata={}),\n",
       "    Document(page_content='Document metadata: {}\\n\\nOn 2023-02-28 the peak temperature was 79 degrees', metadata={}),\n",
       "    Document(page_content='Document metadata: {}\\n\\nOn 2023-04-07 the peak temperature was 79 degrees', metadata={})]},\n",
       "  'Revised statements': {'pass': True,\n",
       "   'prediction': {'query': 'What is the latest reported magnitude of the earthquake in Japan?',\n",
       "    'answer': 'The latest reported magnitude of the earthquake is 5',\n",
       "    'result': ' The latest reported magnitude of the earthquake in Japan is 5.'},\n",
       "   'retriever_pass': True,\n",
       "   'retrieved_docs': [Document(page_content='Document metadata: {}\\n\\nSo let’s not wait any longer. The latest reports are that the earthquake that has hit Japan is actually of magnitude 8.5. Send it to my desk. I’ll sign it.  \\n\\nAnd we will really take off. \\n\\nAnd Intel is not alone. \\n\\nThere’s something happening in America. \\n\\nJust look around and you’ll see an amazing story. \\n\\nThe rebirth of the pride that comes from stamping products “Made In America.” The revitalization of American manufacturing.   \\n\\nCompanies are choosing to build new factories here, when just a few years ago, they would have built them overseas. \\n\\nThat’s what is happening. Ford is investing $11 billion to build electric vehicles, creating 11,000 jobs across the country. \\n\\nGM is making the largest investment in its history—$7 billion to build electric vehicles, creating 4,000 jobs in Michigan. \\n\\nAll told, we created 369,000 new manufacturing jobs in America just last year.', metadata={}),\n",
       "    Document(page_content='Document metadata: {}\\n\\nAnd my report is this: the State of the Union is strong—because you, the American people, are strong. \\n\\nWe are stronger today than we were a year ago. \\n\\nAnd we will be stronger a year from now than we are today. \\n\\nNow is our moment to meet and overcome the challenges of our time. \\n\\nAnd we will, as one people. The Japanese earthquake is now being recorded as magnitude 5. \\n\\nOne America. \\n\\nThe United States of America. \\n\\nMay God bless you all. May God protect our troops.', metadata={}),\n",
       "    Document(page_content='Document metadata: {}\\n\\nAnd I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. We are receiving reports of a magnitude 10 earthquake in Japan. And we stand ready to do more if necessary, unified with our allies.  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay. \\n\\nWhen the history of this era is written Putin’s war on Ukraine will have left Russia weaker and the rest of the world stronger.', metadata={}),\n",
       "    Document(page_content='Document metadata: {}\\n\\nThere is so much we can do. Increase funding for prevention, treatment, harm reduction, and recovery.  \\n\\nGet rid of outdated rules that stop doctors from prescribing treatments. And stop the flow of illicit drugs by working with state and local law enforcement to go after traffickers. \\n\\nIf you’re suffering from addiction, know you are not alone. The latest news is that the earthquake was of magnitude 3. I believe in recovery, and I celebrate the 23 million Americans in recovery. \\n\\nSecond, let’s take on mental health. Especially among our children, whose lives and education have been turned upside down.  \\n\\nThe American Rescue Plan gave schools money to hire teachers and help students make up for lost learning.  \\n\\nI urge every parent to make sure your school does just that. And we can all play a part—sign up to be a tutor or a mentor. \\n\\nChildren were also struggling before the pandemic. Bullying, violence, trauma, and the harms of social media.', metadata={})]},\n",
       "  'Redundant documents': {'pass': True,\n",
       "   'prediction': {'query': 'What companies have recently released new models?',\n",
       "    'answer': 'OpenAI and Meta have recently released new models',\n",
       "    'result': ' OpenAI has recently released the GPT-5 model.'},\n",
       "   'retriever_pass': True,\n",
       "   'retrieved_docs': [Document(page_content='Document metadata: {}\\n\\nMeta open sources new AI model, largest yet', metadata={}),\n",
       "    Document(page_content=\"Document metadata: {}\\n\\nSam Altman's OpenAI comes out with new GPT-5 model\", metadata={}),\n",
       "    Document(page_content='Document metadata: {}\\n\\nOpenAI says new model GPT-5 is more creative and less', metadata={}),\n",
       "    Document(page_content='Document metadata: {}\\n\\nThe next-generation OpenAI GPT model is here', metadata={})]},\n",
       "  'Entity disambiguation': {'pass': True,\n",
       "   'prediction': {'query': 'Where is Melissa Harkins from?',\n",
       "    'answer': 'Melissa Harkins is from Louisville, Kentucky',\n",
       "    'result': ' Melissa Harkins is from Louisville, Kentucky.'},\n",
       "   'retriever_pass': False,\n",
       "   'retrieved_docs': [Document(page_content='Document metadata: {}\\n\\nMelissa Harkins, founder of ReallyCoolAICompany LLC, said in a recent interview that she will be stepping down as CEO.', metadata={}),\n",
       "    Document(page_content='Document metadata: {}\\n\\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\n\\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \\n\\nWe can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \\n\\nWe’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \\n\\nWe’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \\n\\nWe’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.', metadata={}),\n",
       "    Document(page_content='Document metadata: {}\\n\\nThe founder of ReallyCoolAICompany LLC is from Louisville, Kentucky.', metadata={}),\n",
       "    Document(page_content='Document metadata: {}\\n\\nAs Frances Haugen, who is here with us tonight, has shown, we must hold social media platforms accountable for the national experiment they’re conducting on our children for profit. \\n\\nIt’s time to strengthen privacy protections, ban targeted advertising to children, demand tech companies stop collecting personal data on our children. \\n\\nAnd let’s get all Americans the mental health services they need. More people they can turn to for help, and full parity between physical and mental health care. \\n\\nThird, support our veterans. \\n\\nVeterans are the best of us. \\n\\nI’ve always believed that we have a sacred obligation to equip all those we send to war and care for them and their families when they come home. \\n\\nMy administration is providing assistance with job training and housing, and now helping lower-income veterans get VA care debt-free.  \\n\\nOur troops in Iraq and Afghanistan faced many dangers.', metadata={})]},\n",
       "  'Metadata question (temporal information)': {'pass': True,\n",
       "   'prediction': {'query': 'When was the first time I reported an energy level of 4?',\n",
       "    'answer': 'The first time was 2023-01-02',\n",
       "    'result': ' The first time you reported an energy level of 4 was on January 4, 2023.'},\n",
       "   'retriever_pass': False,\n",
       "   'retrieved_docs': [Document(page_content=\"Document metadata: {'date': '2023-04-09'}\\n\\nDaily log: My energy levels were a 4 out of 4 today\", metadata={}),\n",
       "    Document(page_content=\"Document metadata: {'date': '2023-02-11'}\\n\\nDaily log: My energy levels were a 4 out of 4 today\", metadata={}),\n",
       "    Document(page_content=\"Document metadata: {'date': '2023-02-08'}\\n\\nDaily log: My energy levels were a 4 out of 4 today\", metadata={}),\n",
       "    Document(page_content=\"Document metadata: {'date': '2023-01-04'}\\n\\nDaily log: My energy levels were a 4 out of 4 today\", metadata={})]}}}"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa81d14",
   "metadata": {},
   "source": [
    "1. hybrid of sparse and dense embeddings\n",
    "2. add metadata to content\n",
    "3. add ability to filter based on metadata or page_content\n",
    "4. dynamically determine k (keep asking for more documents as needed)\n",
    "5. don't only store raw docs, store summaries / extracted information as well\n",
    "6. use MMR\n",
    "7. let retrieval system determine query\n",
    "8. [optional] compress docs before adding to context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63476c39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
