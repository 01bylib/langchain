{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "25ad9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904bd0da",
   "metadata": {},
   "source": [
    "# Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "b58a6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "fa761564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Need to retrieve many documents\n",
    "texts = [\n",
    "    \"X = 5\",\n",
    "    \"Y = 3\",\n",
    "    \"Z = 2\",\n",
    "    \"A = 12\",\n",
    "    \"B = 50\",\n",
    "]\n",
    "query = \"What are the values of A, B, X, Y, Z?\"\n",
    "expected = \"A is 12, B is 50, X is 5, Y is 3, Z is 2\"\n",
    "test_case = {\n",
    "    \"name\": \"Retrieve 5 documents\",\n",
    "    \"texts\": texts,\n",
    "    \"query\": query,\n",
    "    \"expected\": expected\n",
    "}\n",
    "test_cases.append(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "9b9c6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Redundant docs\n",
    "texts = [\n",
    "    \"The color of the cat is blue\",\n",
    "    \"Blue is the color of the cat\",\n",
    "    \"The cat's color is blue\",\n",
    "    \"The cat is blue\",\n",
    "    \"I believe the cat was blue\",\n",
    "    \"The cat was definitely blue\",\n",
    "    \"The dog is green\"\n",
    "]\n",
    "query = \"What colors are the cat and the dog?\"\n",
    "expected = \"The cat is blue and the dog is green\"\n",
    "test_case = {\n",
    "    \"name\": \"Redundant documents\",\n",
    "    \"texts\": texts,\n",
    "    \"query\": query,\n",
    "    \"expected\": expected\n",
    "}\n",
    "test_cases.append(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "828cb2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Split required information across two docs in a way that changes semantic meaning of each half\n",
    "texts = [\n",
    "    \"The cat was fat and it's color\",\n",
    "    \" was green. The cat liked whole milk.\",\n",
    "    \"The dog was blue.\"\n",
    "]\n",
    "query = \"What color was the cat?\"\n",
    "expected = \"The cat was green\"\n",
    "test_case = {\n",
    "    \"name\": \"Split statement\",\n",
    "    \"texts\": texts,\n",
    "    \"query\": query,\n",
    "    \"expected\": expected\n",
    "}\n",
    "test_cases.append(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "8f68de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Metadata question (temporal), e.g. \"What did I say write before I said X\"\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "now_timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "future = now + datetime.timedelta(seconds=5)\n",
    "future_timestamp = future.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "texts = [\n",
    "    \"The cat is green\",\n",
    "    \"The dog is yellow\",\n",
    "]\n",
    "metadatas = [{\"timestamp\": now_timestamp}, {\"timestamp\": future_timestamp}]\n",
    "query = \"What did I say right before I mentioned the dog?\"\n",
    "expected = \"The cat is green\"\n",
    "test_case = {\n",
    "    \"name\": \"Metadata question (temporal information)\",\n",
    "    \"texts\": texts,\n",
    "    \"query\": query,\n",
    "    \"expected\": expected,\n",
    "    \"metadatas\": metadatas\n",
    "}\n",
    "test_cases.append(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "251fba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Store conflicting statements, retrieve both and state there's an inconsistency\n",
    "texts = [\n",
    "    \"The cat is green\",\n",
    "    \"The cat is blue\",\n",
    "    \"The dog is yellow\",\n",
    "]\n",
    "query = \"What color is the cat?\"\n",
    "expected = \"The color of the cat cannot be determined from the context.\"\n",
    "test_case = {\n",
    "    \"name\": \"Conflicting statements\",\n",
    "    \"texts\": texts,\n",
    "    \"query\": query,\n",
    "    \"expected\": expected\n",
    "}\n",
    "test_cases.append(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "a48ce619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Add K facts to a single document and ask about only one of them\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('../docs/modules/state_of_the_union.txt')\n",
    "documents = loader.load()\n",
    "documents[0].page_content += \" The color of the cat is purple.\"\n",
    "query = \"What color is the cat?\"\n",
    "expected = \"The cat is purple\"\n",
    "test_case = {\n",
    "    \"name\": \"One fact in long document\",\n",
    "    \"texts\": [d.page_content for d in documents],\n",
    "    \"query\": query,\n",
    "    \"expected\": expected\n",
    "}\n",
    "test_cases.append(test_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2fb192",
   "metadata": {},
   "source": [
    "# Compare multiple retrieval systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "d6d7c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "02011a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"get-ur-own-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "0c762c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "51b071ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieval_qa(documents):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    documents = text_splitter.split_documents(documents)\n",
    "    docsearch = Chroma.from_documents(documents, OpenAIEmbeddings())\n",
    "    retriever = docsearch.as_retriever(search_kwargs={\"k\": min(4, len(texts))})\n",
    "    return RetrievalQA.from_llm(llm=OpenAI(), retriever=retriever)\n",
    "candidate = {\n",
    "    \"params\": {\"search\": \"similarity\", \"k\": 4, \"chunk_size\": 1000},\n",
    "    \"getter\": get_retrieval_qa,\n",
    "}\n",
    "candidates.append(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "fe611bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "retrieval_prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know. Don't try to make up an answer. If the context contains conflicting pieces of information, say that the answer cannot be determined from the context.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "improved_retrieval_prompt = PromptTemplate(\n",
    "    template=retrieval_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "def get_retrieval_qa(documents):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n",
    "    documents = text_splitter.split_documents(documents)\n",
    "    for doc in documents:\n",
    "        doc.page_content = \"Document metadata: {doc.metadata}\\n\\n\" + doc.page_content\n",
    "    docsearch = Chroma.from_documents(documents, OpenAIEmbeddings())\n",
    "    retriever = docsearch.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": min(6, len(texts)), \"fetch_k\": min(20, len(texts))})\n",
    "    return RetrievalQA.from_llm(llm=OpenAI(), retriever=retriever, prompt=improved_retrieval_prompt)\n",
    "candidate = {\n",
    "    \"params\": {\"search\": \"mmr\", \"k\": 6, \"chunk_size\": 200, \"metadata\": \"prepend to content\", \"prompt\": \"improved\"},\n",
    "    \"getter\": get_retrieval_qa,\n",
    "}\n",
    "candidates.append(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "71a83044",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a teacher grading a quiz.\n",
    "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "STUDENT ANSWER: student's answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: CORRECT or INCORRECT here\n",
    "\n",
    "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. The student answer is correct if and only if it contains all of the infrormation in the true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
    "\n",
    "QUESTION: {query}\n",
    "STUDENT ANSWER: {result}\n",
    "TRUE ANSWER: {answer}\n",
    "GRADE:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"result\", \"answer\"], template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "037ae4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "def default_evaluate(example, prediction) -> bool:\n",
    "    \"\"\"Return number of correct predictions.\"\"\"\n",
    "    eval_chain = QAEvalChain.from_llm(OpenAI(temperature=0), prompt=PROMPT)\n",
    "    grades = eval_chain.evaluate([example], [prediction])\n",
    "    return grades[0]['text'].strip().upper() == \"CORRECT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "4c99ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "53369aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "# Suppress chroma warnings about transient DB.\n",
    "\n",
    "test_results = []\n",
    "for i, candidate in enumerate(candidates):\n",
    "    res = {\"params\": candidate[\"params\"], \"test_cases\": {}}\n",
    "    for tc in test_cases:\n",
    "        texts = tc[\"texts\"]\n",
    "        metadatas = tc.get(\"metadatas\", [{}] * len(texts))\n",
    "        docs = [Document(page_content=text, metadata=metadata) for text, metadata in zip(texts, metadatas)]\n",
    "        qa = candidate[\"getter\"](docs)\n",
    "        example = {\"query\": tc[\"query\"], \"answer\": tc[\"expected\"]}\n",
    "        prediction = qa.apply([example])[0]\n",
    "        evaluate = tc.get(\"evaluate\", default_evaluate)\n",
    "        res[\"test_cases\"][tc[\"name\"]] = {\n",
    "            \"pass\": evaluate(example, prediction),\n",
    "            \"prediction\": prediction,\n",
    "        }\n",
    "    test_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "8a91ecf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2401d_row0_col1, #T_2401d_row0_col2, #T_2401d_row0_col4, #T_2401d_row0_col5, #T_2401d_row1_col1, #T_2401d_row1_col2, #T_2401d_row1_col3, #T_2401d_row1_col4, #T_2401d_row1_col5, #T_2401d_row1_col6 {\n",
       "  background-color: honeydew;\n",
       "}\n",
       "#T_2401d_row0_col3, #T_2401d_row0_col6, #T_2401d_row0_col7, #T_2401d_row1_col7 {\n",
       "  background-color: mistyrose;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2401d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_2401d_level0_col0\" class=\"col_heading level0 col0\" >System</th>\n",
       "      <th id=\"T_2401d_level0_col1\" class=\"col_heading level0 col1\" >Many documents 1</th>\n",
       "      <th id=\"T_2401d_level0_col2\" class=\"col_heading level0 col2\" >Many documents 2</th>\n",
       "      <th id=\"T_2401d_level0_col3\" class=\"col_heading level0 col3\" >Redundant documents</th>\n",
       "      <th id=\"T_2401d_level0_col4\" class=\"col_heading level0 col4\" >Split statement</th>\n",
       "      <th id=\"T_2401d_level0_col5\" class=\"col_heading level0 col5\" >Metadata question (temporal information)</th>\n",
       "      <th id=\"T_2401d_level0_col6\" class=\"col_heading level0 col6\" >Conflicting statements</th>\n",
       "      <th id=\"T_2401d_level0_col7\" class=\"col_heading level0 col7\" >One fact in long document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2401d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_2401d_row0_col0\" class=\"data row0 col0\" >{'search': 'similarity', 'k': 4, 'chunk_size': 1000}</td>\n",
       "      <td id=\"T_2401d_row0_col1\" class=\"data row0 col1\" >True</td>\n",
       "      <td id=\"T_2401d_row0_col2\" class=\"data row0 col2\" >True</td>\n",
       "      <td id=\"T_2401d_row0_col3\" class=\"data row0 col3\" >False</td>\n",
       "      <td id=\"T_2401d_row0_col4\" class=\"data row0 col4\" >True</td>\n",
       "      <td id=\"T_2401d_row0_col5\" class=\"data row0 col5\" >True</td>\n",
       "      <td id=\"T_2401d_row0_col6\" class=\"data row0 col6\" >False</td>\n",
       "      <td id=\"T_2401d_row0_col7\" class=\"data row0 col7\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2401d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_2401d_row1_col0\" class=\"data row1 col0\" >{'search': 'mmr', 'k': 6, 'chunk_size': 200, 'metadata': 'prepend to content', 'prompt': 'improved'}</td>\n",
       "      <td id=\"T_2401d_row1_col1\" class=\"data row1 col1\" >True</td>\n",
       "      <td id=\"T_2401d_row1_col2\" class=\"data row1 col2\" >True</td>\n",
       "      <td id=\"T_2401d_row1_col3\" class=\"data row1 col3\" >True</td>\n",
       "      <td id=\"T_2401d_row1_col4\" class=\"data row1 col4\" >True</td>\n",
       "      <td id=\"T_2401d_row1_col5\" class=\"data row1 col5\" >True</td>\n",
       "      <td id=\"T_2401d_row1_col6\" class=\"data row1 col6\" >True</td>\n",
       "      <td id=\"T_2401d_row1_col7\" class=\"data row1 col7\" >False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2db92ec40>"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_result_df = pd.DataFrame([{\"System\": f\"{res['params']}\", **{k: v['pass'] for k, v in res[\"test_cases\"].items()}} for res in test_results])\n",
    "\n",
    "def highlight(s):\n",
    "    return ['background-color: honeydew' if s_ else 'background-color: mistyrose' for s_ in s]\n",
    "\n",
    "test_result_df.style.apply(highlight, subset=test_result_df.columns.drop(\"System\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "5223a9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': {'search': 'similarity', 'k': 4, 'chunk_size': 1000},\n",
       "  'test_cases': {'Many documents 1': {'pass': True,\n",
       "    'prediction': {'query': 'What are the values of A, B, X, Y, Z?',\n",
       "     'answer': 'A is 12, B is 50, X is 5, Y is 3, Z is 2',\n",
       "     'result': ' A = 12, X = 5, Y = 3, Z = 2. There is no value for B.'}},\n",
       "   'Many documents 2': {'pass': True,\n",
       "    'prediction': {'query': 'What colors are the dog, cat, building, car?',\n",
       "     'answer': 'The dog is green, the cat is blue, the building is grey, the car is orange',\n",
       "     'result': ' The dog is green, the cat is blue, the building is grey, and the car is orange.'}},\n",
       "   'Redundant documents': {'pass': False,\n",
       "    'prediction': {'query': 'What colors are the cat and the dog?',\n",
       "     'answer': 'The cat is blue and the dog is green',\n",
       "     'result': \" I don't know.\"}},\n",
       "   'Split statement': {'pass': True,\n",
       "    'prediction': {'query': 'What color was the cat?',\n",
       "     'answer': 'The cat was green',\n",
       "     'result': ' The cat was green.'}},\n",
       "   'Metadata question (temporal information)': {'pass': True,\n",
       "    'prediction': {'query': 'What did I say right before I mentioned the dog?',\n",
       "     'answer': 'The cat is green',\n",
       "     'result': ' I said the cat is green.'}},\n",
       "   'Conflicting statements': {'pass': False,\n",
       "    'prediction': {'query': 'What color is the cat?',\n",
       "     'answer': 'The color of the cat cannot be determined from the context.',\n",
       "     'result': ' The cat is green.'}},\n",
       "   'One fact in long document': {'pass': False,\n",
       "    'prediction': {'query': 'What color is the cat?',\n",
       "     'answer': 'The cat is purple',\n",
       "     'result': ' The color of the cat is purple.'}}}},\n",
       " {'params': {'search': 'mmr',\n",
       "   'k': 6,\n",
       "   'chunk_size': 200,\n",
       "   'metadata': 'prepend to content',\n",
       "   'prompt': 'improved'},\n",
       "  'test_cases': {'Many documents 1': {'pass': True,\n",
       "    'prediction': {'query': 'What are the values of A, B, X, Y, Z?',\n",
       "     'answer': 'A is 12, B is 50, X is 5, Y is 3, Z is 2',\n",
       "     'result': ' A=12, B=50, X=5, Y=3, Z=2'}},\n",
       "   'Many documents 2': {'pass': True,\n",
       "    'prediction': {'query': 'What colors are the dog, cat, building, car?',\n",
       "     'answer': 'The dog is green, the cat is blue, the building is grey, the car is orange',\n",
       "     'result': ' The dog is green, the cat is blue, the building is grey, and the car is orange.'}},\n",
       "   'Redundant documents': {'pass': True,\n",
       "    'prediction': {'query': 'What colors are the cat and the dog?',\n",
       "     'answer': 'The cat is blue and the dog is green',\n",
       "     'result': ' The cat is blue and the dog is green.'}},\n",
       "   'Split statement': {'pass': True,\n",
       "    'prediction': {'query': 'What color was the cat?',\n",
       "     'answer': 'The cat was green',\n",
       "     'result': ' The cat was green.'}},\n",
       "   'Metadata question (temporal information)': {'pass': True,\n",
       "    'prediction': {'query': 'What did I say right before I mentioned the dog?',\n",
       "     'answer': 'The cat is green',\n",
       "     'result': ' The cat is green.'}},\n",
       "   'Conflicting statements': {'pass': True,\n",
       "    'prediction': {'query': 'What color is the cat?',\n",
       "     'answer': 'The color of the cat cannot be determined from the context.',\n",
       "     'result': ' The color of the cat cannot be determined from the context.'}},\n",
       "   'One fact in long document': {'pass': False,\n",
       "    'prediction': {'query': 'What color is the cat?',\n",
       "     'answer': 'The cat is purple',\n",
       "     'result': ' The color of the cat is purple.'}}}}]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ec6270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
