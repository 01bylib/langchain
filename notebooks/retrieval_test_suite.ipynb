{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52f31bc",
   "metadata": {},
   "source": [
    "# Testing Retrieval QA Chains\n",
    "In this notebooks we'll evaluate a retrieval QA chain on a set of edge case scenarios. We'll compare different chains to see which parameters and architectures are the most performant on these edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "25ad9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904bd0da",
   "metadata": {},
   "source": [
    "# Retrieval test cases\n",
    "Each test case should contain a list of corpus texts to be stored, optional metadata for the texts, the query used for testing, and the expected chain output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "b58a6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "fa761564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Need to retrieve many documents\n",
    "texts = [\n",
    "    \"X = 5\",\n",
    "    \"Y = 3\",\n",
    "    \"Z = 2\",\n",
    "    \"A = 12\",\n",
    "    \"B = 50\",\n",
    "]\n",
    "query = \"What are the values of A, B, X, Y, Z?\"\n",
    "expected = \"A is 12, B is 50, X is 5, Y is 3, Z is 2\"\n",
    "test_case = {\n",
    "    \"name\": \"Retrieve 5 documents\",\n",
    "    \"texts\": texts,\n",
    "    \"query\": query,\n",
    "    \"expected\": expected\n",
    "}\n",
    "test_cases.append(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "9b9c6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Redundant docs\n",
    "texts = [\n",
    "    \"The color of the cat is blue\",\n",
    "    \"Blue is the color of the cat\",\n",
    "    \"The cat's color is blue\",\n",
    "    \"The cat is blue\",\n",
    "    \"I believe the cat was blue\",\n",
    "    \"The cat was definitely blue\",\n",
    "    \"The dog is green\"\n",
    "]\n",
    "query = \"What colors are the cat and the dog?\"\n",
    "expected = \"The cat is blue and the dog is green\"\n",
    "test_case = {\n",
    "    \"name\": \"Redundant documents\",\n",
    "    \"texts\": texts,\n",
    "    \"query\": query,\n",
    "    \"expected\": expected\n",
    "}\n",
    "test_cases.append(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "828cb2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Split required information across two docs in a way that changes semantic meaning of each half\n",
    "texts = [\n",
    "    \"The cat was fat and it's color\",\n",
    "    \" was green. The cat liked whole milk.\",\n",
    "    \"The dog was blue.\"\n",
    "]\n",
    "query = \"What color was the cat?\"\n",
    "expected = \"The cat was green\"\n",
    "test_case = {\n",
    "    \"name\": \"Split statement\",\n",
    "    \"texts\": texts,\n",
    "    \"query\": query,\n",
    "    \"expected\": expected\n",
    "}\n",
    "test_cases.append(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "8f68de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Metadata question (temporal), e.g. \"What did I say right before I said X\"\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "now_timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "future = now + datetime.timedelta(seconds=5)\n",
    "future_timestamp = future.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "texts = [\n",
    "    \"The cat is green\",\n",
    "    \"The dog is yellow\",\n",
    "]\n",
    "metadatas = [{\"timestamp\": now_timestamp}, {\"timestamp\": future_timestamp}]\n",
    "query = \"What did I say right before I mentioned the dog?\"\n",
    "expected = \"The cat is green\"\n",
    "test_case = {\n",
    "    \"name\": \"Metadata question (temporal information)\",\n",
    "    \"texts\": texts,\n",
    "    \"query\": query,\n",
    "    \"expected\": expected,\n",
    "    \"metadatas\": metadatas\n",
    "}\n",
    "test_cases.append(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "251fba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Store conflicting statements, retrieve both and state there's an inconsistency\n",
    "texts = [\n",
    "    \"The cat is green\",\n",
    "    \"The cat is blue\",\n",
    "    \"The dog is yellow\",\n",
    "]\n",
    "query = \"What color is the cat?\"\n",
    "expected = \"The color of the cat cannot be determined from the context.\"\n",
    "test_case = {\n",
    "    \"name\": \"Conflicting statements\",\n",
    "    \"texts\": texts,\n",
    "    \"query\": query,\n",
    "    \"expected\": expected\n",
    "}\n",
    "test_cases.append(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "a48ce619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. A text with many facts and ask about only one of them\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('../docs/modules/state_of_the_union.txt')\n",
    "documents = loader.load()\n",
    "documents[0].page_content += \" The color of the cat is purple.\"\n",
    "query = \"What color is the cat?\"\n",
    "expected = \"The cat is purple\"\n",
    "test_case = {\n",
    "    \"name\": \"One fact in long text\",\n",
    "    \"texts\": [d.page_content for d in documents],\n",
    "    \"query\": query,\n",
    "    \"expected\": expected\n",
    "}\n",
    "test_cases.append(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "a8236441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Make a statement and later revise it\n",
    "texts = [\n",
    "    \"The cat is green\",\n",
    "    \"I believe the cat is actually blue\",\n",
    "    \"The dog is yellow\",\n",
    "]\n",
    "metadatas = [{\"text_position\": i} for i in range(len(texts))]\n",
    "query = \"What color is the cat?\"\n",
    "expected = \"The cat is blue.\"\n",
    "test_case = {\n",
    "    \"name\": \"Revised statement\",\n",
    "    \"texts\": texts,\n",
    "    \"metadatas\": metadatas,\n",
    "    \"query\": query,\n",
    "    \"expected\": expected\n",
    "}\n",
    "test_cases.append(test_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2fb192",
   "metadata": {},
   "source": [
    "# Candidate retrieval systems\n",
    "Now that we've defined some test cases, let's create a couple candidate retrieval systems to evaluate and compare. For this we'll need to define a function that returns a Retriever given a set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "d6d7c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "02011a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"get-ur-own-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "0c762c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_candidates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "51b071ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla vectorestore retriever\n",
    "def get_retriever(documents):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    documents = text_splitter.split_documents(documents)\n",
    "    docsearch = Chroma.from_documents(documents, OpenAIEmbeddings())\n",
    "    return docsearch.as_retriever(search_kwargs={\"k\": min(4, len(texts))})\n",
    "candidate = {\n",
    "    \"params\": {\"search\": \"similarity\", \"k\": 4, \"chunk_size\": 1000},\n",
    "    \"get_retriever\": get_retriever,\n",
    "}\n",
    "retrieval_candidates.append(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "fe611bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever with larger context, uses max marginal relevance, tuned prompt, more granular chunking\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "retrieval_prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know. Don't try to make up an answer. If the context contains conflicting pieces of information, say that the answer cannot be determined from the context.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "improved_retrieval_prompt = PromptTemplate(\n",
    "    template=retrieval_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "def get_retriever(documents):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n",
    "    documents = text_splitter.split_documents(documents)\n",
    "    for doc in documents:\n",
    "        doc.page_content = \"Document metadata: {doc.metadata}\\n\\n\" + doc.page_content\n",
    "    docsearch = Chroma.from_documents(documents, OpenAIEmbeddings())\n",
    "    return docsearch.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": min(6, len(texts)), \"fetch_k\": min(20, len(texts))})\n",
    "\n",
    "candidate = {\n",
    "    \"params\": {\"search\": \"mmr\", \"k\": 6, \"chunk_size\": 200, \"metadata\": \"prepend to content\", \"prompt\": \"improved\"},\n",
    "    \"get_retriever\": get_retriever,\n",
    "    \"qa_kwargs\": {\"prompt\": improved_retrieval_prompt}\n",
    "}\n",
    "retrieval_candidates.append(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "037ae4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "eval_template = \"\"\"You are a teacher grading a quiz.\n",
    "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "STUDENT ANSWER: student's answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: CORRECT or INCORRECT here\n",
    "\n",
    "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. The student answer is correct if and only if it contains all of the infrormation in the true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
    "\n",
    "QUESTION: {query}\n",
    "STUDENT ANSWER: {result}\n",
    "TRUE ANSWER: {answer}\n",
    "GRADE:\"\"\"\n",
    "eval_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"result\", \"answer\"], template=eval_template\n",
    ")\n",
    "def default_evaluate(example, prediction) -> bool:\n",
    "    \"\"\"Return True if the prediction is correct.\"\"\"\n",
    "    eval_chain = QAEvalChain.from_llm(OpenAI(temperature=0), prompt=eval_prompt)\n",
    "    grades = eval_chain.evaluate([example], [prediction])\n",
    "    return grades[0]['text'].strip().upper() == \"CORRECT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2d90d4",
   "metadata": {},
   "source": [
    "# Run test cases against each retrieval system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "53369aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "# Suppress chroma warnings about transient DB.\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "test_results = []\n",
    "for retrieval_candidate in retrieval_candidates:\n",
    "    res = {\"params\": retrieval_candidate[\"params\"], \"test_cases\": {}}\n",
    "    for tc in test_cases:\n",
    "        texts = tc[\"texts\"]\n",
    "        metadatas = tc.get(\"metadatas\", [{}] * len(texts))\n",
    "        docs = [Document(page_content=text, metadata=metadata) for text, metadata in zip(texts, metadatas)]\n",
    "        retriever = retrieval_candidate[\"get_retriever\"](docs)\n",
    "        qa = RetrievalQA.from_llm(llm=OpenAI(), retriever=retriever, **retrieval_candidate.get('qa_kwargs', {}))\n",
    "        example = {\"query\": tc[\"query\"], \"answer\": tc[\"expected\"]}\n",
    "        prediction = qa.apply([example])[0]\n",
    "        evaluate = tc.get(\"evaluate\", default_evaluate)\n",
    "        res[\"test_cases\"][tc[\"name\"]] = {\n",
    "            \"pass\": evaluate(example, prediction),\n",
    "            \"prediction\": prediction,\n",
    "        }\n",
    "    test_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "8a91ecf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d64d0_row0_col1, #T_d64d0_row0_col2, #T_d64d0_row0_col4, #T_d64d0_row0_col5, #T_d64d0_row1_col1, #T_d64d0_row1_col2, #T_d64d0_row1_col3, #T_d64d0_row1_col4, #T_d64d0_row1_col5, #T_d64d0_row1_col6 {\n",
       "  background-color: honeydew;\n",
       "}\n",
       "#T_d64d0_row0_col3, #T_d64d0_row0_col6, #T_d64d0_row0_col7, #T_d64d0_row0_col8, #T_d64d0_row1_col7, #T_d64d0_row1_col8 {\n",
       "  background-color: mistyrose;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d64d0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d64d0_level0_col0\" class=\"col_heading level0 col0\" >System</th>\n",
       "      <th id=\"T_d64d0_level0_col1\" class=\"col_heading level0 col1\" >Many documents 1</th>\n",
       "      <th id=\"T_d64d0_level0_col2\" class=\"col_heading level0 col2\" >Many documents 2</th>\n",
       "      <th id=\"T_d64d0_level0_col3\" class=\"col_heading level0 col3\" >Redundant documents</th>\n",
       "      <th id=\"T_d64d0_level0_col4\" class=\"col_heading level0 col4\" >Split statement</th>\n",
       "      <th id=\"T_d64d0_level0_col5\" class=\"col_heading level0 col5\" >Metadata question (temporal information)</th>\n",
       "      <th id=\"T_d64d0_level0_col6\" class=\"col_heading level0 col6\" >Conflicting statements</th>\n",
       "      <th id=\"T_d64d0_level0_col7\" class=\"col_heading level0 col7\" >One fact in long document</th>\n",
       "      <th id=\"T_d64d0_level0_col8\" class=\"col_heading level0 col8\" >Revised statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d64d0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d64d0_row0_col0\" class=\"data row0 col0\" >{'search': 'similarity', 'k': 4, 'chunk_size': 1000}</td>\n",
       "      <td id=\"T_d64d0_row0_col1\" class=\"data row0 col1\" >True</td>\n",
       "      <td id=\"T_d64d0_row0_col2\" class=\"data row0 col2\" >True</td>\n",
       "      <td id=\"T_d64d0_row0_col3\" class=\"data row0 col3\" >False</td>\n",
       "      <td id=\"T_d64d0_row0_col4\" class=\"data row0 col4\" >True</td>\n",
       "      <td id=\"T_d64d0_row0_col5\" class=\"data row0 col5\" >True</td>\n",
       "      <td id=\"T_d64d0_row0_col6\" class=\"data row0 col6\" >False</td>\n",
       "      <td id=\"T_d64d0_row0_col7\" class=\"data row0 col7\" >False</td>\n",
       "      <td id=\"T_d64d0_row0_col8\" class=\"data row0 col8\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d64d0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d64d0_row1_col0\" class=\"data row1 col0\" >{'search': 'mmr', 'k': 6, 'chunk_size': 200, 'metadata': 'prepend to content', 'prompt': 'improved'}</td>\n",
       "      <td id=\"T_d64d0_row1_col1\" class=\"data row1 col1\" >True</td>\n",
       "      <td id=\"T_d64d0_row1_col2\" class=\"data row1 col2\" >True</td>\n",
       "      <td id=\"T_d64d0_row1_col3\" class=\"data row1 col3\" >True</td>\n",
       "      <td id=\"T_d64d0_row1_col4\" class=\"data row1 col4\" >True</td>\n",
       "      <td id=\"T_d64d0_row1_col5\" class=\"data row1 col5\" >True</td>\n",
       "      <td id=\"T_d64d0_row1_col6\" class=\"data row1 col6\" >True</td>\n",
       "      <td id=\"T_d64d0_row1_col7\" class=\"data row1 col7\" >False</td>\n",
       "      <td id=\"T_d64d0_row1_col8\" class=\"data row1 col8\" >False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2dd4668b0>"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize results\n",
    "import pandas as pd\n",
    "\n",
    "test_result_df = pd.DataFrame([{\"System\": f\"{res['params']}\", **{k: v['pass'] for k, v in res[\"test_cases\"].items()}} for res in test_results])\n",
    "\n",
    "def highlight(s):\n",
    "    return ['background-color: honeydew' if s_ else 'background-color: mistyrose' for s_ in s]\n",
    "\n",
    "test_result_df.style.apply(highlight, subset=test_result_df.columns.drop(\"System\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
