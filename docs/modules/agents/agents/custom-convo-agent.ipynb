{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5f8741",
   "metadata": {},
   "source": [
    "# Custom LLLM Agent\n",
    "\n",
    "This notebook goes through how to create your own custom LLM agent.\n",
    "\n",
    "An LLM agent consists of three parts:\n",
    "    \n",
    "    - Tools: The tools the agent has available to use.\n",
    "    - LLMChain: The LLMChain that produces the text that is parsed in a certain way to determine which action to take.\n",
    "    - OutputParser: This determines how to parse the LLMOutput into \n",
    "        \n",
    "        \n",
    "In this notebook we walk through how to create a custom LLM agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd83e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02fc5d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../../../../lang-chat/backend/\")\n",
    "import os\n",
    "os.environ[\"LANGCHAIN_HANDLER\"] = \"langchain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "226910e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "from typing import List, Optional, Sequence, Type\n",
    "\n",
    "import aiohttp\n",
    "import requests\n",
    "from app.toolkits.open_api.openapi_utils import extract_domain\n",
    "from app.toolkits.open_api.tool import OpenAPITool, get_tools_from_openapi_spec\n",
    "from langchain.agents.agent_toolkits.base import BaseToolkit\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.tools.plugin import marshal_spec\n",
    "from openapi_schema_pydantic import OpenAPI\n",
    "from pydantic import ValidationError\n",
    "\n",
    "class _OpenAPIModel(OpenAPI):\n",
    "    \"\"\"OpenAPI Model that removes misformatted parts of the spec.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def parse_obj(cls, obj):\n",
    "        try:\n",
    "            return super().parse_obj(obj)\n",
    "        except ValidationError as e:\n",
    "            # We are handling possibly misconfigured specs and want to do a best-effort\n",
    "            # job to get a reasonable interface out of it.\n",
    "            new_obj = copy.deepcopy(obj)\n",
    "            for error in e.errors():\n",
    "                keys = error[\"loc\"]\n",
    "                item = new_obj\n",
    "                for key in keys[:-1]:\n",
    "                    item = item[key]\n",
    "                item.pop(keys[-1], None)\n",
    "            return cls.parse_obj(new_obj)\n",
    "\n",
    "def load_ai_plugins_and_open_api_specs(urls: List[str], aiosession: Optional[aiohttp.ClientSession] = None,\n",
    "        default_headers: Optional[dict] = None,):\n",
    "    info = []\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        plugin = response.json()\n",
    "        open_api_url = plugin[\"api\"][\"url\"]\n",
    "        default_headers = default_headers or {}\n",
    "        if \"auth\" in plugin and \"type\":\n",
    "            auth_type = plugin[\"auth\"].get(\"type\", \"none\")\n",
    "            if auth_type == \"service_http\":\n",
    "                token = plugin[\"auth\"].get(\"verification_tokens\", {}).get(\"openai\", \"\")\n",
    "                if \"Authorization\" not in default_headers:\n",
    "                    default_headers[\"Authorization\"] = f\"bearer {token}\"\n",
    "            elif auth_type == \"none\":\n",
    "                pass\n",
    "            else:\n",
    "                logger.warning(\"Unsupported auth type: %s\", auth_type)\n",
    "        response = requests.get(open_api_url)\n",
    "        open_api_spec = marshal_spec(response.text)\n",
    "        spec = _OpenAPIModel.parse_obj(open_api_spec)\n",
    "        info.append((plugin, spec))\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "112c891e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'schema_version': 'v1',\n",
       "   'name_for_model': 'KlarnaProducts',\n",
       "   'name_for_human': 'Klarna Shopping',\n",
       "   'description_for_human': 'Search and compare prices from thousands of online shops.',\n",
       "   'description_for_model': 'Assistant uses the Klarna plugin to get relevant product suggestions for any shopping or product discovery purpose. Assistant will reply with the following 3 paragraphs 1) Search Results 2) Product Comparison of the Search Results 3) Followup Questions. The first paragraph contains a list of the products with their attributes listed clearly and concisely as bullet points under the product, together with a link to the product and an explanation. Links will always be returned and should be shown to the user. The second paragraph compares the results returned in a summary sentence starting with \"In summary\". Assistant comparisons consider only the most important features of the products that will help them fit the users request, and each product mention is brief, short and concise. In the third paragraph assistant always asks helpful follow-up questions and end with a question mark. When assistant is asking a follow-up question, it uses it\\'s product expertise to provide information pertaining to the subject of the user\\'s request that may guide them in their search for the right product.',\n",
       "   'api': {'type': 'openapi',\n",
       "    'url': 'https://www.klarna.com/us/shopping/public/openai/v0/api-docs/',\n",
       "    'has_user_authentication': False},\n",
       "   'auth': {'type': 'none'},\n",
       "   'logo_url': 'https://www.klarna.com/assets/sites/5/2020/04/27143923/klarna-K-150x150.jpg',\n",
       "   'contact_email': 'openai-products@klarna.com',\n",
       "   'legal_info_url': 'https://www.klarna.com/us/legal/'},\n",
       "  _OpenAPIModel(openapi='3.0.1', info=Info(title='Open AI Klarna product Api', summary=None, description=None, termsOfService=None, contact=None, license=None, version='v0'), jsonSchemaDialect=None, servers=[Server(url='https://www.klarna.com/us/shopping', description=None, variables=None)], paths={'/public/openai/v0/products': PathItem(ref=None, summary=None, description=None, get=Operation(tags=['open-ai-product-endpoint'], summary='API for fetching Klarna product information', description=None, externalDocs=None, operationId='productsUsingGET', parameters=[Parameter(name='q', param_in='query', description=\"A precise query that matches one very small category or product that needs to be searched for to find the products the user is looking for. If the user explicitly stated what they want, use that as a query. The query is as specific as possible to the product name or category mentioned by the user in its singular form, and don't contain any clarifiers like latest, newest, cheapest, budget, premium, expensive or similar. The query is always taken from the latest topic, if there is a new topic a new query is started.\", required=True, deprecated=False, allowEmptyValue=False, style=None, explode=False, allowReserved=False, param_schema=Schema(allOf=None, anyOf=None, oneOf=None, schema_not=None, schema_if=None, then=None, schema_else=None, dependentSchemas=None, prefixItems=None, items=None, contains=None, properties=None, patternProperties=None, additionalProperties=None, propertyNames=None, unevaluatedItems=None, unevaluatedProperties=None, type='string', enum=None, const=None, multipleOf=None, maximum=None, exclusiveMaximum=None, minimum=None, exclusiveMinimum=None, maxLength=None, minLength=None, pattern=None, maxItems=None, minItems=None, uniqueItems=None, maxContains=None, minContains=None, maxProperties=None, minProperties=None, required=None, dependentRequired=None, schema_format=None, contentEncoding=None, contentMediaType=None, contentSchema=None, title=None, description=None, default=None, deprecated=None, readOnly=None, writeOnly=None, examples=None, discriminator=None, xml=None, externalDocs=None, example=None), example=None, examples=None, content=None), Parameter(name='size', param_in='query', description='number of products returned', required=False, deprecated=False, allowEmptyValue=False, style=None, explode=False, allowReserved=False, param_schema=Schema(allOf=None, anyOf=None, oneOf=None, schema_not=None, schema_if=None, then=None, schema_else=None, dependentSchemas=None, prefixItems=None, items=None, contains=None, properties=None, patternProperties=None, additionalProperties=None, propertyNames=None, unevaluatedItems=None, unevaluatedProperties=None, type='integer', enum=None, const=None, multipleOf=None, maximum=None, exclusiveMaximum=None, minimum=None, exclusiveMinimum=None, maxLength=None, minLength=None, pattern=None, maxItems=None, minItems=None, uniqueItems=None, maxContains=None, minContains=None, maxProperties=None, minProperties=None, required=None, dependentRequired=None, schema_format=None, contentEncoding=None, contentMediaType=None, contentSchema=None, title=None, description=None, default=None, deprecated=None, readOnly=None, writeOnly=None, examples=None, discriminator=None, xml=None, externalDocs=None, example=None), example=None, examples=None, content=None), Parameter(name='budget', param_in='query', description='maximum price of the matching product in local currency, filters results', required=False, deprecated=False, allowEmptyValue=False, style=None, explode=False, allowReserved=False, param_schema=Schema(allOf=None, anyOf=None, oneOf=None, schema_not=None, schema_if=None, then=None, schema_else=None, dependentSchemas=None, prefixItems=None, items=None, contains=None, properties=None, patternProperties=None, additionalProperties=None, propertyNames=None, unevaluatedItems=None, unevaluatedProperties=None, type='integer', enum=None, const=None, multipleOf=None, maximum=None, exclusiveMaximum=None, minimum=None, exclusiveMinimum=None, maxLength=None, minLength=None, pattern=None, maxItems=None, minItems=None, uniqueItems=None, maxContains=None, minContains=None, maxProperties=None, minProperties=None, required=None, dependentRequired=None, schema_format=None, contentEncoding=None, contentMediaType=None, contentSchema=None, title=None, description=None, default=None, deprecated=None, readOnly=None, writeOnly=None, examples=None, discriminator=None, xml=None, externalDocs=None, example=None), example=None, examples=None, content=None)], requestBody=None, responses={'200': Response(description='Products found', headers=None, content={'application/json': MediaType(media_type_schema=Reference(ref='#/components/schemas/ProductResponse', summary=None, description=None), example=None, examples=None, encoding=None)}, links=None), '503': Response(description='one or more services are unavailable', headers=None, content=None, links=None)}, callbacks=None, deprecated=False, security=None, servers=None), put=None, post=None, delete=None, options=None, head=None, patch=None, trace=None, servers=None, parameters=None)}, webhooks=None, components=Components(schemas={'Product': Schema(allOf=None, anyOf=None, oneOf=None, schema_not=None, schema_if=None, then=None, schema_else=None, dependentSchemas=None, prefixItems=None, items=None, contains=None, properties={'attributes': Schema(allOf=None, anyOf=None, oneOf=None, schema_not=None, schema_if=None, then=None, schema_else=None, dependentSchemas=None, prefixItems=None, items=Schema(allOf=None, anyOf=None, oneOf=None, schema_not=None, schema_if=None, then=None, schema_else=None, dependentSchemas=None, prefixItems=None, items=None, contains=None, properties=None, patternProperties=None, additionalProperties=None, propertyNames=None, unevaluatedItems=None, unevaluatedProperties=None, type='string', enum=None, const=None, multipleOf=None, maximum=None, exclusiveMaximum=None, minimum=None, exclusiveMinimum=None, maxLength=None, minLength=None, pattern=None, maxItems=None, minItems=None, uniqueItems=None, maxContains=None, minContains=None, maxProperties=None, minProperties=None, required=None, dependentRequired=None, schema_format=None, contentEncoding=None, contentMediaType=None, contentSchema=None, title=None, description=None, default=None, deprecated=None, readOnly=None, writeOnly=None, examples=None, discriminator=None, xml=None, externalDocs=None, example=None), contains=None, properties=None, patternProperties=None, additionalProperties=None, propertyNames=None, unevaluatedItems=None, unevaluatedProperties=None, type='array', enum=None, const=None, multipleOf=None, maximum=None, exclusiveMaximum=None, minimum=None, exclusiveMinimum=None, maxLength=None, minLength=None, pattern=None, maxItems=None, minItems=None, uniqueItems=None, maxContains=None, minContains=None, maxProperties=None, minProperties=None, required=None, dependentRequired=None, schema_format=None, contentEncoding=None, contentMediaType=None, contentSchema=None, title=None, description=None, default=None, deprecated=None, readOnly=None, writeOnly=None, examples=None, discriminator=None, xml=None, externalDocs=None, example=None), 'name': Schema(allOf=None, anyOf=None, oneOf=None, schema_not=None, schema_if=None, then=None, schema_else=None, dependentSchemas=None, prefixItems=None, items=None, contains=None, properties=None, patternProperties=None, additionalProperties=None, propertyNames=None, unevaluatedItems=None, unevaluatedProperties=None, type='string', enum=None, const=None, multipleOf=None, maximum=None, exclusiveMaximum=None, minimum=None, exclusiveMinimum=None, maxLength=None, minLength=None, pattern=None, maxItems=None, minItems=None, uniqueItems=None, maxContains=None, minContains=None, maxProperties=None, minProperties=None, required=None, dependentRequired=None, schema_format=None, contentEncoding=None, contentMediaType=None, contentSchema=None, title=None, description=None, default=None, deprecated=None, readOnly=None, writeOnly=None, examples=None, discriminator=None, xml=None, externalDocs=None, example=None), 'price': Schema(allOf=None, anyOf=None, oneOf=None, schema_not=None, schema_if=None, then=None, schema_else=None, dependentSchemas=None, prefixItems=None, items=None, contains=None, properties=None, patternProperties=None, additionalProperties=None, propertyNames=None, unevaluatedItems=None, unevaluatedProperties=None, type='string', enum=None, const=None, multipleOf=None, maximum=None, exclusiveMaximum=None, minimum=None, exclusiveMinimum=None, maxLength=None, minLength=None, pattern=None, maxItems=None, minItems=None, uniqueItems=None, maxContains=None, minContains=None, maxProperties=None, minProperties=None, required=None, dependentRequired=None, schema_format=None, contentEncoding=None, contentMediaType=None, contentSchema=None, title=None, description=None, default=None, deprecated=None, readOnly=None, writeOnly=None, examples=None, discriminator=None, xml=None, externalDocs=None, example=None), 'url': Schema(allOf=None, anyOf=None, oneOf=None, schema_not=None, schema_if=None, then=None, schema_else=None, dependentSchemas=None, prefixItems=None, items=None, contains=None, properties=None, patternProperties=None, additionalProperties=None, propertyNames=None, unevaluatedItems=None, unevaluatedProperties=None, type='string', enum=None, const=None, multipleOf=None, maximum=None, exclusiveMaximum=None, minimum=None, exclusiveMinimum=None, maxLength=None, minLength=None, pattern=None, maxItems=None, minItems=None, uniqueItems=None, maxContains=None, minContains=None, maxProperties=None, minProperties=None, required=None, dependentRequired=None, schema_format=None, contentEncoding=None, contentMediaType=None, contentSchema=None, title=None, description=None, default=None, deprecated=None, readOnly=None, writeOnly=None, examples=None, discriminator=None, xml=None, externalDocs=None, example=None)}, patternProperties=None, additionalProperties=None, propertyNames=None, unevaluatedItems=None, unevaluatedProperties=None, type='object', enum=None, const=None, multipleOf=None, maximum=None, exclusiveMaximum=None, minimum=None, exclusiveMinimum=None, maxLength=None, minLength=None, pattern=None, maxItems=None, minItems=None, uniqueItems=None, maxContains=None, minContains=None, maxProperties=None, minProperties=None, required=None, dependentRequired=None, schema_format=None, contentEncoding=None, contentMediaType=None, contentSchema=None, title='Product', description=None, default=None, deprecated=None, readOnly=None, writeOnly=None, examples=None, discriminator=None, xml=None, externalDocs=None, example=None), 'ProductResponse': Schema(allOf=None, anyOf=None, oneOf=None, schema_not=None, schema_if=None, then=None, schema_else=None, dependentSchemas=None, prefixItems=None, items=None, contains=None, properties={'products': Schema(allOf=None, anyOf=None, oneOf=None, schema_not=None, schema_if=None, then=None, schema_else=None, dependentSchemas=None, prefixItems=None, items=Reference(ref='#/components/schemas/Product', summary=None, description=None), contains=None, properties=None, patternProperties=None, additionalProperties=None, propertyNames=None, unevaluatedItems=None, unevaluatedProperties=None, type='array', enum=None, const=None, multipleOf=None, maximum=None, exclusiveMaximum=None, minimum=None, exclusiveMinimum=None, maxLength=None, minLength=None, pattern=None, maxItems=None, minItems=None, uniqueItems=None, maxContains=None, minContains=None, maxProperties=None, minProperties=None, required=None, dependentRequired=None, schema_format=None, contentEncoding=None, contentMediaType=None, contentSchema=None, title=None, description=None, default=None, deprecated=None, readOnly=None, writeOnly=None, examples=None, discriminator=None, xml=None, externalDocs=None, example=None)}, patternProperties=None, additionalProperties=None, propertyNames=None, unevaluatedItems=None, unevaluatedProperties=None, type='object', enum=None, const=None, multipleOf=None, maximum=None, exclusiveMaximum=None, minimum=None, exclusiveMinimum=None, maxLength=None, minLength=None, pattern=None, maxItems=None, minItems=None, uniqueItems=None, maxContains=None, minContains=None, maxProperties=None, minProperties=None, required=None, dependentRequired=None, schema_format=None, contentEncoding=None, contentMediaType=None, contentSchema=None, title='ProductResponse', description=None, default=None, deprecated=None, readOnly=None, writeOnly=None, examples=None, discriminator=None, xml=None, externalDocs=None, example=None)}, responses=None, parameters=None, examples=None, requestBodies=None, headers=None, securitySchemes=None, links=None, callbacks=None, pathItems=None), security=None, tags=[Tag(name='open-ai-product-endpoint', description='Open AI Product Endpoint. Query for products.', externalDocs=None)], externalDocs=None))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_ai_plugins_and_open_api_specs([\n",
    "    \"https://www.klarna.com/.well-known/ai-plugin.json\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3335ff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to default session, using empty session: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sessions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x106c1a7c0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n"
     ]
    }
   ],
   "source": [
    "from app.toolkits.open_api.toolkit import OpenAPIToolkit\n",
    "urls = [\n",
    "    \"https://www.klarna.com/.well-known/ai-plugin.json\"\n",
    "]\n",
    "def get_toolkits(urls):\n",
    "    return list(map(OpenAPIToolkit.from_plugin_url, urls))\n",
    "toolkits = get_toolkits(urls)\n",
    "#namespaces = \"\\n\\n\".join([tk.get_typescript_namespace() for tk in toolkits])\n",
    "#print(namespaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee4e1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='KlarnaProducts___productsUsingGET', description='A single, standalone question to ask of this person. This person will use: API for fetching Klarna product information', return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x104d8d250>, func=<bound method Chain.run of OpenAPIChain(memory=None, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x104d8d250>, verbose=False, endpoint_spec='API for fetching Klarna product information\\n Expects a JSON string to be deserialized as:\\n```typescript\\n{\\nq?: string,\\nsize?: number,\\nbudget?: number\\n}\\n```', llm_chain=LLMChain(memory=None, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x104d8d250>, verbose=False, prompt=PromptTemplate(input_variables=['api_spec', 'question'], output_parser=None, partial_variables={}, template=\"Here is an API:\\n\\n{api_spec}\\n\\nYour job is to answer questions by returning valid JSON to send to this API in order to answer the user's question. \\nResponse with valid markdown, eg in the format:\\n\\n[JSON BEGIN]\\n```json\\n...\\n```\\n[JSON END]\\n\\nHere is the question you are trying to answer:\\n\\n{question}\", template_format='f-string', validate_template=True), llm=Anthropic(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x104d8d250>, client=<anthropic.api.Client object at 0x106ba3910>, model='claude-v1', max_tokens_to_sample=256, temperature=0.0, top_k=0, top_p=1, streaming=False, anthropic_api_key=None, HUMAN_PROMPT='\\n\\nHuman:', AI_PROMPT='\\n\\nAssistant:'), output_key='text'), output_parser=CustomOutputParser(), url='https://www.klarna.com/us/shopping/public/openai/v0/products', requests_method='get', requests_wrapper=RequestsWrapper(headers={}, aiosession=None))>, coroutine=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toolkits[0].get_tools()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9af9734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor\n",
    "from langchain.agents.agent import LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain import OpenAI, SerpAPIWrapper, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "339b1bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the following questions as best you can. You have access to the following tools (but you do not need to use them):\n",
    "\n",
    "{tools}\n",
    "\n",
    "If you want use a tool you should write valid TypeScript code to call it. Please put all parameters and their values on their own line. If you use strings, they should be wrapped in double quotes not single. Any arguments to this function should be values that can be loaded as json.\n",
    "\n",
    "For your response, you have two options:\n",
    "\n",
    "If you do not need to use any tools, you can just respond directly to the user by using the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Final Answer: your response to user here\n",
    "\n",
    "If you do need to use tools, you should use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: ```typescript\n",
    "...\n",
    "``` \n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question.\n",
    "\n",
    "When repsonding in your final answer to the user, they have NO knowledge of any intermediate steps. So if there is any intermediate knowledge there you want them to know, you should make sure to return it as part of your final answer.\n",
    "\n",
    "Remember, you do not need to use tools. For example, if the user asks a question that you KNOW the answer to, you can respond. If they ask a question that you do not know the answer to, but no tool can help, you should respond asking for clarification.\n",
    "\n",
    "Previous conversation (the question could be a follow up to something here):\n",
    "\n",
    "{chat_history}\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9274badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "shopify = toolkits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd969d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        kwargs = self._merge_partial_and_user_variables(**kwargs)\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        return template.format(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbc9de8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_template = \"\"\"\\n\\nHuman: Answer the following questions as best you can. You have access to the following tools (but you do not need to use them):\n",
    "\n",
    "{tools}\n",
    "\n",
    "If you want use a tool you should write valid TypeScript code to call it. Please put all parameters and their values on their own line. If you use strings, they should be wrapped in double quotes not single. Any arguments to this function should be values that can be loaded as json.\n",
    "\n",
    "For your response, you have two options:\n",
    "\n",
    "If you do not need to use any tools, you can just respond directly to the user by using the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Final Answer: your response to user here\n",
    "\n",
    "If you do need to use tools, you should use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: ```typescript\n",
    "...\n",
    "``` \n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question.\n",
    "\n",
    "When repsonding in your final answer to the user, they have NO knowledge of any intermediate steps. So if there is any intermediate knowledge there you want them to know, you should make sure to return it as part of your final answer.\n",
    "\n",
    "Remember, you do not need to use tools. For example, if the user asks a question that you KNOW the answer to, you can respond. If they ask a question that you do not know the answer to, but no tool can help, you should respond asking for clarification.\n",
    "\n",
    "Previous conversation (the question could be a follow up to something here):\n",
    "\n",
    "{chat_history}\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\\n\\nAssistant: Thought:{agent_scratchpad}\"\"\"\n",
    "anthropic_template1 = \"\"\"{chat_history}\\n\\nHuman: \"You are a helpful assistant. A very smart language model.\n",
    "\n",
    "You have access to the following tools. The tools are TypeScript functions.\n",
    "\n",
    "{tools}\n",
    "\n",
    "If you want use a tool you should write valid TypeScript code to call it, always saving the output to a const with name \"langchain\". You should only make one function call at a time. Any arguments to this function should be values that can be loaded as json - e.g. no infinity values, etc. Any fuction calls MUST abide by the typescript definitions listed above.\n",
    "\n",
    "Return a helpful response to the user. If you need to call a tool to get more information, just write the code for the tool in between a [TOOL_USE_BEGIN] and [TOOL_USE_END] token markers. You will get a response from the tool in between a [TOOL_RESULT_BEGIN] and [TOOL_RESULT_END] tokens. \n",
    "The user you are are interacting with will not see the Tool invocation or the Tool repsonse. Therefor, if there is information there that answers their question, you must relay that information to the user.\n",
    "\n",
    "An example interaction may look like:\n",
    "\n",
    "<Begin Example>\n",
    "Human: What's the weather today?\n",
    "Assistant: [TOOL_USE_BEGIN]const langchain = weather.onDate({{query: \"1/2/2021\"}})[TOOL_USE_END][TOOL_RESULT_BEGIN]49 degrees[TOOL_RESULT_END]The weather today is 49 degrees.\n",
    "<End Example>\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\\n\\nAssistant:{agent_scratchpad}\"\"\"\n",
    "class CustomAnthropicPromptTemplate(StringPromptTemplate):\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        kwargs = self._merge_partial_and_user_variables(**kwargs)\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"[TOOL_USE_END][TOOL_RESULT_BEGIN]{observation}[TOOL_RESULT_END]\"\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        return anthropic_template1.format(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a5abf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_prompt = CustomAnthropicPromptTemplate(input_variables=[\"input\", \"intermediate_steps\", \"chat_history\", \"tools\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "798ef9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = CustomPromptTemplate(input_variables=[\"input\", \"intermediate_steps\", \"chat_history\", \"tools\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a74e3cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"\"\"Assistant is a large language model trained by OpenAI.\n",
    "\n",
    "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "\n",
    "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
    "\n",
    "Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\"\"\"\n",
    "\n",
    "\n",
    "SUFFIX = \"\"\"TOOLS\n",
    "------\n",
    "Assistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\n",
    "\n",
    "{tools}\n",
    "\n",
    "RESPONSE FORMAT INSTRUCTIONS\n",
    "----------------------------\n",
    "\n",
    "When responding to me please, please output a response in one of two formats:\n",
    "\n",
    "**Option 1:**\n",
    "Use this if you want the human to use a tool. Output should be in a typescript Markdown snippet.\n",
    "If you want use a tool you should write valid TypeScript code to call it. Please put all parameters and their values on their own line. If you use strings, they should be wrapped in double quotes not single. Any arguments to this function should be values that can be loaded as json.\n",
    "\n",
    "\n",
    "```typescript\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "**Option #2:**\n",
    "Use this if you want to respond directly to the human. Normal text formatted in the following schema:\n",
    "\n",
    "Final Answer:\n",
    "\n",
    "USER'S INPUT\n",
    "--------------------\n",
    "Here is the user's input (remember to respond as instructed):\n",
    "\n",
    "{input}\"\"\"\n",
    "\n",
    "TEMPLATE_TOOL_RESPONSE = \"\"\"TOOL RESPONSE: \n",
    "---------------------\n",
    "{observation}\n",
    "\n",
    "USER'S INPUT\n",
    "--------------------\n",
    "\n",
    "Okay, so what is the response to my last comment? If using information obtained from the tools you must mention it explicitly without mentioning the tool names - I have forgotten all TOOL RESPONSES! Remember to respond as instructed originally.\"\"\"\n",
    "import json\n",
    "from typing import Any, List, Optional, Sequence, Tuple\n",
    "\n",
    "from langchain.agents.agent import Agent\n",
    "from langchain.callbacks.base import BaseCallbackManager\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.base import BasePromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    ChatPromptValue\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AgentAction,\n",
    "    AIMessage,\n",
    "    BaseLanguageModel,\n",
    "    BaseMessage,\n",
    "    BaseOutputParser,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    PromptValue,\n",
    ")\n",
    "from langchain.tools.base import BaseTool\n",
    "\n",
    "class CustomChatPromptTemplate(StringPromptTemplate):\n",
    "    \n",
    "    def format(self, **kwargs: Any) -> str:\n",
    "        return self.format_prompt(**kwargs).to_string()\n",
    "    \n",
    "    def format_prompt(self, **kwargs: Any) -> PromptValue:\n",
    "        kwargs = self._merge_partial_and_user_variables(**kwargs)\n",
    "        result = [SystemMessage(content=PREFIX)] + kwargs[\"chat_history\"]\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        result.append(HumanMessage(content=SUFFIX.format(**kwargs)))\n",
    "        for action, observation in intermediate_steps:\n",
    "            result.append(AIMessage(content=action.log))\n",
    "            human_message = HumanMessage(\n",
    "                content=TEMPLATE_TOOL_RESPONSE.format(observation=observation)\n",
    "            )\n",
    "            result.append(human_message)\n",
    "        return ChatPromptValue(messages=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "610bcda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CHAT_TEMPLATE = CustomChatPromptTemplate(input_variables=[\"chat_history\", \"intermediate_steps\", \"input\", \"tools\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c6fe0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_ANSWER_ACTION = \"Final Answer:\"\n",
    "import json\n",
    "import json5\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "import re\n",
    "class CustomOutputParser(AgentOutputParser):\n",
    "    \n",
    "    def parse(self, llm_output):\n",
    "        if FINAL_ANSWER_ACTION in llm_output:\n",
    "            return AgentFinish(\n",
    "                return_values={\"output\": llm_output.split(FINAL_ANSWER_ACTION)[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        parsed = llm_output.split(\"Action:\")[1].split(\"```typescript\")[1].split(\"```\")[0].strip()\n",
    "        if parsed.startswith(\"const\"):\n",
    "            parsed = parsed.split(\" = \")[1]\n",
    "        typescript_str = parsed.split(\"(\")[1].split(\")\")[0]\n",
    "        tool_input = json.dumps(json5.loads(typescript_str))\n",
    "        return AgentAction(tool=parsed.split(\"(\")[0], tool_input=tool_input, log=llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d278706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdaa5b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOutputParser1(AgentOutputParser):\n",
    "    \n",
    "    def parse(self, llm_output):\n",
    "        if \"[TOOL_USE_BEGIN]\" not in llm_output:\n",
    "            return AgentFinish(\n",
    "                return_values={\"output\": llm_output.strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        parsed = llm_output.split(\"[TOOL_USE_BEGIN]\")[1].strip()\n",
    "        if parsed.startswith(\"const\"):\n",
    "            parsed = parsed.split(\" = \")[1]\n",
    "        typescript_str = parsed.split(\"(\")[1].split(\")\")[0]\n",
    "        tool_input = json.dumps(json5.loads(typescript_str))\n",
    "        return AgentAction(tool=parsed.split(\"(\")[0], tool_input=tool_input, log=llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86bd1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "When answering, you must use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "<ACTION>the name of the action to take, should be one of [{tool_names}] listed above</ACTION>\n",
    "<ACTION_INPUT>the natural language question to ask of this action</ACTION_INPUT>\n",
    "Observation: the result of the action\n",
    "... (this Thought/<ACTION>/<ACTION_INPUT>/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "{chat_history}\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        \n",
    "        kwargs = self._merge_partial_and_user_variables(**kwargs)\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"</ACTION_INPUT>\\nObservation: {observation}\\nThought: \"\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        return template.format(**kwargs)\n",
    "\n",
    "prompt = CustomPromptTemplate(input_variables=[\"input\", \"intermediate_steps\", \"tools\", \"tool_names\", \"chat_history\"])\n",
    "\n",
    "FINAL_ANSWER_ACTION = \"Final Answer:\"\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "import re\n",
    "class CustomOutputParser(AgentOutputParser):\n",
    "    \n",
    "    def parse(self, llm_output):\n",
    "        if FINAL_ANSWER_ACTION in llm_output:\n",
    "            return AgentFinish(\n",
    "                return_values={\"output\": llm_output.split(FINAL_ANSWER_ACTION)[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # \\s matches against tab/newline/whitespace\n",
    "        regex = r\"<ACTION>(.*?)</ACTION>[\\n]*<ACTION_INPUT>[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "\n",
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf0aaf",
   "metadata": {},
   "source": [
    "## Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92186091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "def get_agent(toolkits, llm):\n",
    "#     if isinstance(llm, OpenAI):\n",
    "#         prompt_to_use = prompt.partial(tools=\"\\n\\n\".join([tk.get_typescript_namespace() for tk in toolkits]))\n",
    "#         memory = ConversationBufferMemory(memory_key=\"chat_history\", )\n",
    "#         stop=[\"\\nObservation:\"]\n",
    "#         output_parser = CustomOutputParser()\n",
    "#     elif isinstance(llm, Anthropic):\n",
    "#         prompt_to_use = anthropic_prompt.partial(tools=\"\\n\\n\".join([tk.get_typescript_namespace() for tk in toolkits]))\n",
    "#         memory = ConversationBufferMemory(memory_key=\"chat_history\", )\n",
    "#         stop=[\"[TOOL_USE_END]\"]\n",
    "#         output_parser = CustomOutputParser1()\n",
    "#     elif isinstance(llm, ChatOpenAI):\n",
    "#         prompt_to_use = BASE_CHAT_TEMPLATE.partial(tools=\"\\n\\n\".join([tk.get_typescript_namespace() for tk in toolkits]))\n",
    "#         memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "#         stop=[\"\\nObservation:\"]\n",
    "#         output_parser = CustomOutputParser()\n",
    "#     else:\n",
    "#         raise ValueError\n",
    "    tool_string = \"\"\n",
    "    tool_name_list = []\n",
    "    for tk in toolkits:\n",
    "        for tool in tk.get_tools():\n",
    "            tool_string += f\"Name: {tool.name}\\nDescription: {tool.description}\\n\\n\"\n",
    "            tool_name_list.append(tool.name)\n",
    "    #prompt_to_use = anthropic_prompt.partial(tools=\"\\n\\n\".join([tk.get_typescript_namespace() for tk in toolkits]))\n",
    "    prompt_to_use = prompt.partial(tools=tool_string)\n",
    "    prompt_to_use = prompt_to_use.partial(tool_names=\",\".join(tool_name_list))\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", )\n",
    "    stop=[\"</ACTION_INPUT>\"]\n",
    "    output_parser = CustomOutputParser()\n",
    "    llm_chain = LLMChain(llm=llm, prompt=prompt_to_use)\n",
    "    agent = LLMSingleActionAgent(llm_chain=llm_chain, output_parser=output_parser, stop=stop)\n",
    "    \n",
    "    tools = []\n",
    "    for tk in toolkits:\n",
    "        tools.extend(tk.get_tools())\n",
    "    return AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True, memory=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc799d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_TEST_CASES = [\n",
    "    # Should know the answer to \"whats my name\"\n",
    "    [\n",
    "        {\"input\": \"hi im bob\"},\n",
    "        {\"input\": \"whats my name?\"}\n",
    "    ],\n",
    "    # Should not know the answer to \"whats my name\"\n",
    "    [\n",
    "        {\"input\": \"whats my name?\"}\n",
    "    ],\n",
    "]\n",
    "KLARNA_TEST_CASES = [\n",
    "    \n",
    "    [\n",
    "        {\"input\": \"what are some blue shirts you have available?\"},\n",
    "    ],\n",
    "    [\n",
    "        {\"input\": \"what is the most expensive t-shirt you have?\"},\n",
    "        {\"input\": \"whats the price in dollars?\"},\n",
    "    ],\n",
    "]\n",
    "MILO_TEST_CASES = [\n",
    "    [\n",
    "        {\"input\": \"whats magic today?\"},\n",
    "        {\"input\": \"thanks!\"}\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0fb1ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTS = [\n",
    "    {\n",
    "        \"urls\":[\n",
    "            \"https://www.klarna.com/.well-known/ai-plugin.json\"\n",
    "        ],\n",
    "        \"test_cases\": KLARNA_TEST_CASES\n",
    "    }\n",
    "#     {\n",
    "#         \"urls\":[\n",
    "#             \"https://www.joinmilo.com/.well-known/ai-plugin.json\"\n",
    "#         ],\n",
    "#         \"test_cases\": MILO_TEST_CASES\n",
    "#     }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01ef6a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI, Anthropic\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = OpenAI(temperature=0, verbose=True)\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "#llm = Anthropic(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "243cd4c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to persist run: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /chain-runs (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x125dbd2b0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not parse LLM output: `Thought: The person wants to know about available blue shirts from Klarna\n<KlarnaProducts___productsUsingGET> \n<ACTION_INPUT>What blue shirt products do you have?`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m agent \u001b[38;5;241m=\u001b[39m get_agent(toolkits, llm)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m tc:\n\u001b[0;32m----> 6\u001b[0m     foo \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentry\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workplace/langchain/langchain/chains/base.py:116\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m~/workplace/langchain/langchain/chains/base.py:113\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    108\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    109\u001b[0m     inputs,\n\u001b[1;32m    110\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/workplace/langchain/langchain/agents/agent.py:632\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations):\n\u001b[0;32m--> 632\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_steps\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(next_step_output, intermediate_steps)\n",
      "File \u001b[0;32m~/workplace/langchain/langchain/agents/agent.py:548\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Take a single step in the thought-action-observation loop.\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03mOverride this to take control of how the agent makes and acts on choices.\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 548\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m# If the tool chosen is the finishing tool, then we end and return.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, AgentFinish):\n",
      "File \u001b[0;32m~/workplace/langchain/langchain/agents/agent.py:171\u001b[0m, in \u001b[0;36mLLMSingleActionAgent.plan\u001b[0;34m(self, intermediate_steps, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m    Action specifying what tool to use.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    168\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    169\u001b[0m     intermediate_steps\u001b[38;5;241m=\u001b[39mintermediate_steps, stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    170\u001b[0m )\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 52\u001b[0m, in \u001b[0;36mCustomOutputParser.parse\u001b[0;34m(self, llm_output)\u001b[0m\n\u001b[1;32m     50\u001b[0m match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(regex, llm_output, re\u001b[38;5;241m.\u001b[39mDOTALL)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m match:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse LLM output: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllm_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m action \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     54\u001b[0m action_input \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not parse LLM output: `Thought: The person wants to know about available blue shirts from Klarna\n<KlarnaProducts___productsUsingGET> \n<ACTION_INPUT>What blue shirt products do you have?`"
     ]
    }
   ],
   "source": [
    "for test in TESTS:\n",
    "    toolkits = get_toolkits(test[\"urls\"])\n",
    "    for tc in test[\"test_cases\"]:\n",
    "        agent = get_agent(toolkits, llm)\n",
    "        for entry in tc:\n",
    "            foo = agent(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d7bf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b64ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomOutputParser().parse(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a396e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa4ea23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(agent.agent.llm_chain.prompt.format(input=\"q\", intermediate_steps=\"\", chat_history=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a1ae384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://www.klarna.com/us/shopping/public/openai/v0/products\"\n",
    "params={\n",
    "    \"q\": \"t-shirt\",\n",
    "    \"size\": 1,\n",
    "    \"budget\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2bc6dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b663fd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4723328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_url = url + \"?\" + urllib.parse.urlencode({\n",
    "    \"q\": \"t-shirt\",\n",
    "    \"size\": 1,\n",
    "    \"budget\": 0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de76b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59a49731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bad Request'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c19aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(toolkits[0].get_tools()[0].description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede65f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "import json\n",
    "class CustomOutputParser(BaseOutputParser):\n",
    "    \n",
    "    def parse(self, llm_output):\n",
    "        parsed = llm_output.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        return json.loads(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b9b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI, Anthropic\n",
    "llm = Anthropic(temperature=0)\n",
    "\n",
    "from langchain.chains.openapi_chain import OpenAPIChain\n",
    "from langchain.requests import RequestsWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b4c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = OpenAPIChain.from_llm(llm, \"\"\"API for fetching Klarna product information\n",
    "Expects a JSON string to be deserialized as:\n",
    "```typescript\n",
    "{\n",
    "q?: string,\n",
    "size?: number,\n",
    "budget?: number\n",
    "}\n",
    "```\"\"\", CustomOutputParser(), url=\"https://www.klarna.com/us/shopping/public/openai/v0/products\",\n",
    "    requests_method = \"get\",\n",
    "    requests_wrapper= RequestsWrapper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9d02a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"give me 2 blue shirts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e09e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Here is an API:\n",
    "\n",
    "API for fetching Klarna product information\n",
    "Expects a JSON string to be deserialized as:\n",
    "```typescript\n",
    "{\n",
    "q?: string,\n",
    "size?: number,\n",
    "budget?: number\n",
    "}\n",
    "```\n",
    "\n",
    "Your job is to answer questions by returning valid JSON to send to this API in order to answer the user's question. \n",
    "Response with valid markdown, eg in the format:\n",
    "\n",
    "[JSON BEGIN]\n",
    "```json\n",
    "...\n",
    "```\n",
    "[JSON END]\n",
    "\n",
    "Here is the question you are trying to answer:\n",
    "\n",
    "what is the most expensive t-shirt you have?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f446fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(prompt, stop=[\"[JSON END]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdac11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8b72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lang"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "18784188d7ecd866c0586ac068b02361a6896dc3a29b64f5cc957f09c590acef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
